import numpy as np
import scipy
import matplotlib.pyplot as plt
from scipy.optimize import minimize
import torch
from scipy.misc import derivative
import numdifftools as nd

from scipy.integrate import quadrature

def damped_oscillator(t, y, gamma, k):
    """
    Models a damped oscillator system.

    Inputs:
          t: Float, the current time point.
          y: List, current state of the system.
          gamma: Float, damping coefficient.
          k: Float, stiffness coefficient.

    Outputs:
         [y[1], f_t - gamma*y[1] - k*y[0]]: List, Derivative of the system state.
    """
    f_t = 0  
    #f_t = np.sin(t)
    #print("gamma=",gamma)
    gamma = gamma if isinstance(gamma, float) else gamma[0]
    
    return [y[1], f_t - gamma*y[1] - k*y[0]]

def simulate_observed_data(gamma, k, initial_conditions, ts, N, noise_level_s, noise_level_j):
    """
    Simulates the damped oscillator system over a given time span using solve_ivp.

    Inputs:
      gamma: Float, damping coefficient.
      k: Float, stiffness coefficient.
      initial_conditions: List, initial state of the system.
      ts: List, time span for simulation .
      N: Integer, number of points to evaluate in the time span.
      smooth_noise_level: number, level of random noise in the first half (smooth phase).
      jumpy_noise_level: number, level of random noise in the second half (jumpy phase).
    Outputs:
      (y, yp): list, where y is the position array and yp is the velocity array over the time span.
    """
    
    sol = scipy.integrate.solve_ivp(damped_oscillator, ts, initial_conditions, args=(gamma, k), t_eval=T)
    
    y = sol.y[0]
    yp = sol.y[1]
    #print("y1=",y)
    half = N // 2
    # Apply smooth noise to the first half and jumpy noise to the second half
    noise1 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    
    noise2 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    y += noise1
    #print("noise1=",noise1)
    yp += noise2
    #print("y2=",y)
    return y, yp

def euler_forwarda(gamma, k, y0, t, h):
    """
    Numerically approximates the solution of the damped oscillator using the Euler forward method for a single time point.

    Inputs:
      gamma: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      y0: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    #print("gamma=",gamma)
    y_t_plus_one = y0 + h * np.array(damped_oscillator(t, y0, gamma, k))
    #print(f"Euler Method: gamma={h}, y0={damped_oscillator(t, y0, gamma, k)}, y_t_plus_one={y_t_plus_one}") 
    return y_t_plus_one

def trapezoidal_methoda(gamma, k, y0, t, h ):
    """
    Numerically approximates the solution of the damped oscillator using the trapezoidal method for a single time point.

    Inputs:
      gammas: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      initial_conditions: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    f_n = np.array(damped_oscillator(t, y0, gamma, k))
    y_pred = y0 + h * f_n
    f_n_plus_1 = np.array(damped_oscillator(t + h, y_pred, gamma, k))
    y_t_puls_one = y0 + h / 2 * (f_n + f_n_plus_1)
    return y_t_puls_one

def euler_forward(gamma, k, y0, T,h):
    """
   Numerically approximates the solution of the damped oscillator using the Euler forward method.

   Inputs:
     gamma: Float, damping coefficient.
     k: Float, stiffness coefficient.
     y0: 1D Array, initial state of the system (position and velocity).
     T: Array, time points.

   Outputs:
     y: 2D Array, where each row corresponds to the system states (position and velocity) at each time point.
   """
    y = np.zeros((len(T), 2))
    y[0, :] = y0
    
    if len(T)>1:
        for i, t in enumerate(T[:-1]):
            h = T[i+1] - T[i]
            y[i + 1, :] = y[i, :] + h * np.array(damped_oscillator(t, y[i, :], gamma, k))
            
    return y[-1, :]

def trapezoidal_method(gamma, k, y0, T,h):
    """
    Numerically approximates the solution of the damped oscillator using the trapezoidal method for a single time point.

    Inputs:
      gammas: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      initial_conditions: 2D Array, initial states of the system for each gamma (position and velocity).
      T: Float, Time points.
      

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    y = np.zeros((len(T), 2))
    y[0, :] = y0
    if len(T)>1:
        for i, t in enumerate(T[:-1]):
            h = T[i+1] - T[i]
            f_n = np.array(damped_oscillator(t, y[i, :], gamma, k))
            y_pred = y[i, :] + h * f_n
            f_n_plus_1 = np.array(damped_oscillator(t + h, y_pred, gamma, k))
            y[i + 1, :] = y[i, :] + h / 2 * (f_n + f_n_plus_1)

    return y[-1, :]

def log_likelihood(simulated, observed_y,observed_yp,  noise_level):
    """
    Computes the combined log-likelihood of the observed position data for a given simulated dataset.
  
    Inputs:
        simulated: Array, the simulated data from the model.
        observed_y: Array, observed data for position (y).
        observed_yp: Array, observed data for velocity (y').
        noise_level: Float, standard deviation of noise for the data.


    Outputs:
        log_likelihood: Float, the combined log-likelihood value of the observed data given the simulated model outputs.

    """
    
    residuals_y = observed_y - simulated[0]  
    residuals_yp = observed_yp - simulated[1]  
    
    #print("residuals_yp=", residuals_yp)
    #print("residuals_y=", residuals_y) 
    #print(" simulated[0]=",  simulated)
    ll_y = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level ** 2) - 0.5 / (noise_level ** 2) * (residuals_y ** 2)
    ll_yp = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level ** 2) - 0.5 / (noise_level ** 2) * (residuals_yp ** 2)
    #print("Log-likelihood", ll_y + ll_yp)  
    return ll_y + ll_yp





def gradient_log_likelihood_euler(gamma, observed_y,observed_yp, noise_level, k, y0,t, h):
    """
    Computes the gradient of the log-likelihood with respect to the damping coefficient gamma using the Euler forward method.

    Inputs:
        observed_y: Array of observed position data.
        gamma: Float, the current value of the damping coefficient.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float,times step length.
        noise_level: Float, standard deviation of noise for the data.
        
        Z_t: Normalizing constant.

    Outputs:
        grad_ll: Float, the gradient of the log-likelihood with respect to gamma.
    """
    y = euler_forward(gamma, k, y0, t, h)
    #grad_ll = np.gradient(y, gamma)
    ll_gamma = log_likelihood(y, observed_y,observed_yp, noise_level)
    y_plus_h = euler_forward(gamma + h, k, y0, t, h)
    ll_gamma_h = log_likelihood(y_plus_h, observed_y,observed_yp, noise_level)
    grad_ll = (ll_gamma_h - ll_gamma) / h
    
    return grad_ll




def gradient_log_likelihood_trapezoidal(gamma, observed_y,observed_yp, noise_level, k, y0, t, h):
    """
   Computes the gradient of the log-likelihood with respect to the damping coefficient gamma using the trapezoidal method.

   Inputs:
       observed_y: Array of observed position data.
       gamma: Float, the current value of the damping coefficient.
       k: Float, stiffness coefficient.
       y0: List, initial conditions of the system.
       t: Float, current time.
       h: Float,times step length.
       noise_level: Float, standard deviation of noise for the data.
       Z_t: Normalizing constant.

   Outputs:
       grad_ll: Float, the gradient of the log-likelihood with respect to gamma.
   """
    
    y = trapezoidal_method(gamma, k, y0, t, h)
    
    ll_gamma = log_likelihood(y, observed_y,observed_yp, noise_level)
    y_plus_h = trapezoidal_method(gamma + h, k, y0, t, h)
    ll_gamma_h = log_likelihood(y_plus_h, observed_y,observed_yp, noise_level)
    grad_ll = (ll_gamma_h - ll_gamma) / h
    return grad_ll










def optimize_gamma(log_likelihood, grad_log_likelihood, gamma_init, observed_y, observed_yp,noise_level, k, y0, t, h, method):
    
    result = scipy.optimize.minimize(
        fun=lambda gamma: -log_likelihood(method(gamma, k, y0, t, h), observed_y,observed_yp, noise_level),
        x0=gamma_init,
        jac=lambda gamma: -grad_log_likelihood(gamma, observed_y, observed_yp,noise_level, k, y0, t, h),
        bounds=[(0, 1)],
        method='L-BFGS-B'
    )
    #print(f"Optimization result: {result}")  # Debugging statement
    
    return result.x

    
    




def fisher_information(log_likelihood, gamma, observed_y,observed_yp, noise_level, k, y0, t, h, method):
    """
    Calculates the Fisher information using numerical differentiation.

    Inputs:
        log_likelihood: Function to compute the log-likelihood.
        gamma: Float, the current value of the damping coefficient.
        observed_y: Array of observed position data.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        fisher_info: Float, the Fisher information value.
    """
    
    
    epsilon = 0.00001
    ll_gamma = log_likelihood(method(gamma, k, y0, t, h), observed_y,observed_yp, noise_level)
    #print("ll_gamma =",ll_gamma)
    
    
    if np.isclose(gamma, 0):
        ll_gamma_plus_epsilon = log_likelihood(method(gamma + epsilon, k, y0, t, h), observed_y, observed_yp, noise_level)
        gradient = (ll_gamma_plus_epsilon - ll_gamma) / epsilon
        second_derivative = (ll_gamma_plus_epsilon - 2 * ll_gamma + ll_gamma) / (epsilon ** 2)
    elif np.isclose(gamma, 1):
        ll_gamma_minus_epsilon = log_likelihood(method(gamma - epsilon, k, y0, t, h), observed_y, observed_yp, noise_level)
        gradient = (ll_gamma - ll_gamma_minus_epsilon) / epsilon
        second_derivative = (ll_gamma - 2 * ll_gamma + ll_gamma_minus_epsilon) / (epsilon ** 2)
    else:
        ll_gamma_plus_epsilon = log_likelihood(method(gamma + epsilon, k, y0, t, h), observed_y, observed_yp, noise_level)
        ll_gamma_minus_epsilon = log_likelihood(method(gamma - epsilon, k, y0, t, h), observed_y, observed_yp, noise_level)
        gradient = (ll_gamma_plus_epsilon - ll_gamma_minus_epsilon) / (2 * epsilon)
        second_derivative = (ll_gamma_plus_epsilon - 2 * ll_gamma + ll_gamma_minus_epsilon) / (epsilon ** 2)
        
    if np.isclose(gamma, 0) or np.isclose(gamma, 1):
        fisher = -(second_derivative + 2 * gradient)  # Adjusted Fisher Information including the gradient
    else:
        fisher = -second_derivative
    #print("fisher-",fisher)
    return fisher


    

def find_POST_t(gammas,gamma_init, observed_y,observed_yp, noise_level, N, k, initial_conditions, T, tol, eta_init, c1, c2, M=2):
    """
   Calculates the normalizing constant Z_t(M) after finding the MLE gamma.

   Inputs:
       gamma_init: Float, initial guess for gamma.
       observed_y: Array of observed position data.
       noise_level: Float, standard deviation of noise for the data.
       N: Integer, number of time points.
       k: Float, stiffness coefficient.
       y0: List, initial conditions of the system.
       T: Array, time points for the simulation.
       tol: Float, tolerance for convergence.
       eta_init: Float, initial step size.
       c1: Float, Armijo condition constant.
       c2: Float, curvature condition constant.
       M: Integer, number of methods (default is 2).

   Outputs:
       P_e: Float, the updated normalizing constant for Euler method.
       P_t: Float, the updated normalizing constant for trapezoidal method.
   """
    Z_e = [0.5]
    Z_t = [0.5]
    MLE_e = gamma_init
    MLE_t = gamma_init
    P_e = [0.5]
    P_t = [0.5]
    
    Z_E =[1]
    Z_T = [1]
    P_E = [0.5]
    P_T = [0.5]
    number_of_gammas = 10
    y0 = initial_conditions
    num_internal_steps = 10
    
    
    
    for t in range(1, len(T)):
        #print("len(T)=",len(T))
        #print("time step=",t)
        h = T[t] - T[t - 1]
        
        observed_y_t = observed_y[t]
        observed_yp_t = observed_yp[t]
        
        y0 = [observed_y[t - 1], observed_yp[t - 1]] 
        
        T_internal = np.linspace(T[t - 1], T[t], num_internal_steps + 1)
        #print("T_internal = ",T_internal)
        #print("y00=",y0)        
        MLE_e = optimize_gamma(log_likelihood, gradient_log_likelihood_euler, P_e[-1], observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, euler_forward)
        MLE_t = optimize_gamma(log_likelihood, gradient_log_likelihood_trapezoidal, P_t[-1], observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, trapezoidal_method)
        #print("MLE_e = ", MLE_e)
        #print("MLE_t = ", MLE_t)
        
       

        neg_fisher_e = fisher_information(log_likelihood, MLE_e, observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, euler_forward)
        neg_fisher_t = fisher_information(log_likelihood, MLE_t, observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, trapezoidal_method)
        #print("one_over_neg_Fisher_e = ", one_over_neg_fisher_e)
        #print("one_over_neg_Fisher_t = ", one_over_neg_fisher_t)
        if np.any(neg_fisher_e <= 0) or np.any(neg_fisher_t <= 0):
            break
        posterior_e = scipy.stats.norm(loc=MLE_e, scale=np.sqrt(1 / neg_fisher_e))
        posterior_t = scipy.stats.norm(loc=MLE_t, scale=np.sqrt(1 / neg_fisher_t))
        #print("posterior_e=", posterior_e)
        #print("posterior_t = ", posterior_t)
        
        model_prior_e = (P_e[-1])
        model_prior_t = (P_t[-1])
        
        

        likelihood_e = np.exp(log_likelihood(euler_forward(MLE_e, k, y0, T_internal, h), observed_y_t,observed_yp_t, noise_level))
        likelihood_t = np.exp(log_likelihood(trapezoidal_method(MLE_t, k, y0, T_internal, h), observed_y_t, observed_yp_t,noise_level))

        #print("likelihood_e = ", likelihood_e)
        #print("likelihood_t = ", likelihood_t)
        if np.any(np.isnan(likelihood_e)) or np.any(np.isnan(likelihood_t)):
            print("NaN in likelihoods")
            break
        
        Z_e.append(((likelihood_e ) / posterior_e.pdf(MLE_e))[0])
        Z_t.append(((likelihood_t ) / posterior_t.pdf(MLE_t))[0])
        #print("Z_e=",Z_e)
        #print("Z_t=",Z_t)
        if np.any(np.isnan(Z_e[-1])) or np.any(np.isnan(Z_t[-1])):
            #print("NaN in Z_t or Z_e")
            break
        P_e.append(model_prior_e * Z_e[-1] / (model_prior_e * Z_e[-1] + model_prior_t * Z_t[-1]))
        P_t.append(model_prior_t * Z_t[-1] / (model_prior_e * Z_e[-1] + model_prior_t * Z_t[-1]))
        #print("p_e = ",P_e)
        
        
        Z_E_bayesian_euler = compute_Z_quadrature(observed_y[t], observed_yp[t], noise_level, k, y0, T_internal, h, euler_forward)
        Z_E_bayesian_euler_check = compute_Z_quadrature_E(observed_y[t], observed_yp[t], noise_level, k, y0, T_internal, h)
        #print(f"Large Z-value at time step {t}: Z_E_bayesian_euler = {Z_E_bayesian_euler},  Z_E_bayesian_euler_check  = { Z_E_bayesian_euler_check }  ")
        #if np.any(Z_E_bayesian_euler != Z_E_bayesian_euler_check) :
            #print("something wrong with quadratrue")
            #break
        
        Z_T_bayesian_trapezoidal = compute_Z_quadrature(observed_y[t], observed_yp[t], noise_level, k, y0, T_internal, h, trapezoidal_method)

        Z_E.append(Z_E_bayesian_euler)
        Z_T.append(Z_T_bayesian_trapezoidal)
        
        
        P_E.append([(np.exp(-50*((observed_y_t-y0[0]-h*y0[1])**2+(observed_yp_t-y0[1]+k*h*y0[0]-gamma*h*y0[1])**2)))  / Z_E_bayesian_euler_check for gamma in gammas])
        #P_E.append([np.exp(log_likelihood(euler_forward(gamma, k, y0, T[t - 1], h), observed_y_t, observed_yp_t, noise_level))  / Z_E_bayesian_euler for gamma in gammas])
        P_T.append([np.exp(log_likelihood(trapezoidal_method(gamma, k, y0, T_internal, h), observed_y_t, observed_yp_t, noise_level))  / Z_T_bayesian_trapezoidal for gamma in gammas])

        #print("Z_T=",Z_T)   
        
        threshold = 1000
        if Z_e[-1] > threshold or Z_t[-1] > threshold or t == 10 or t==50 or t ==80:
            
        
            #print(f"Large Z-value at time step {t}: Z_E_bayesian_euler = {Z_E_bayesian_euler}, Z_E_bayesian_euler_check = {Z_E_bayesian_euler_check}  ")
            print(f"Large ZE-value at time step {t}: Z_e = {Z_e[t]} ")
            print(f"Large observed_y-value at time step {t}: observed_y_t = {observed_y_t}, observed_yp_t= {observed_yp_t}  ")
            print(f"Large y-values at time step {t}: y = {y0[0]}, yp = {y0[1]}  ")
            #print(f"Posterior_e = {posterior_e.pdf(MLE_e[0])}, Posterior_t = {posterior_t.pdf(MLE_t[0])} ")
            #print(f" Likelihood_e = {likelihood_e} , Likelihood_t = {likelihood_t}")
            #print(f" one_over_neg_fisher_e = {neg_fisher_e} , one_over_neg_fisher_t = {neg_fisher_t}")
            #print(f"MLE_E = {MLE_e}   MLE_T={MLE_t}" )
            y_ee = euler_forward(MLE_e, k, y0, T_internal, h)
            y_tt = trapezoidal_method(MLE_t, k, y0, T_internal, h)
            residuals_y_e = observed_y[t] - y_ee[0]  
            residuals_yp_e = observed_yp[t] - y_ee[1] 
            #print(f" residuals_y_e = {residuals_y_e} , residuals_yp_e = {residuals_yp_e}")
            residuals_y_t = observed_y[t] - y_tt[0]  
            residuals_yp_t = observed_yp[t] - y_tt[1] 
            #print(f" residuals_y_t = {residuals_y_t} , residuals_yp_t = {residuals_yp_t}")
            
            posterior_e_vals = posterior_e.pdf(gammas)
           
            
            #print(f"posterior_e_vals = {posterior_e_vals}")
                
                
            posterior_e_bayesian_vals = P_E[-1]
            #print(f"posterior_e_bayesian_vals = {posterior_e_bayesian_vals} " )
            
            plt.figure(figsize=(8, 6))
            plt.plot(gammas, posterior_e_vals, label=f'Euler Posterior at t={t}', color='blue')
            plt.plot(gammas, posterior_e_bayesian_vals, label=f'Bayesian Euler Posterior at t={t}', color='green',marker='o', linestyle='--')
            plt.title(f'Posterior Distributions at Time Step {t}')
            plt.xlabel('Gamma')
            plt.ylabel('Posterior (Likelihood Normalized)')
            plt.legend()
            plt.grid(True)
            plt.show()
            
            
            
            plt.figure(figsize=(8, 6))
            plt.plot(gammas, posterior_e_bayesian_vals, label=f'Bayesian Euler Posterior at t={t}', color='green',marker='o', linestyle='--')
            plt.title(f'Posterior Distributions at Time Step {t}')
            plt.xlabel('Gamma')
            plt.ylabel('Posterior (Likelihood Normalized)')
            plt.legend()
            plt.grid(True)
            plt.show()
    return P_e, P_t , P_E, P_T ,Z_E,Z_T,Z_e,Z_t

  

def compute_Z_quadrature(observed_y_t, observed_yp_t,noise_level, k, y0, t, h, method):
    """
    Calculate the normalization constant Z using quadrature integration.

    Inputs:
        observed_y_t: Array, observed data for position (y) at time t.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        Z: Float, the computed normalization constant using quadrature integration.
    """

    integrand = lambda gamma: np.exp(log_likelihood(method(gamma, k, y0, t, h), observed_y_t, observed_yp_t, noise_level))

    Z, _ = quadrature(integrand, 0, 1, tol=1e-8, maxiter=10000)
    return Z

def compute_Z_quadrature_E(observed_y_t, observed_yp_t,noise_level, k, y0, t, h):
    """
    Calculate the normalization constant Z using quadrature integration.

    Inputs:
        observed_y_t: Array, observed data for position (y) at time t.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        Z: Float, the computed normalization constant using quadrature integration.
    """

    integrand = lambda gamma: np.exp((-0.5/(sigma)**2)*((observed_y_t-y0[0]-h*y0[1])**2+(observed_yp_t-y0[1]+k*h*y0[0]-gamma*h*y0[1])**2))
    
    #Z = np.exp(-50*((observed_y_t-y0[0]-h*y0[1])**2+(observed_yp_t-y0[1]+k*h*y0[0])**2))-0.5*y0[1]*(observed_yp_t-y0[1]+k*h*y0[0])+h**2*1/3*(y0[1])**2
    
    
    Z, _ = quadrature(integrand, 0, 1, tol=1e-8, maxiter=10000)
    return Z





if __name__ == '__main__':   
    
    #print('debug=',combined_log_likelihood)
    # Parameters 
    m = 1
    k = 5
    initial_conditions = [1, 0]
    number_of_gammas = 10
    Dimention_of_parameter_space = 1
    gammas = np.linspace(0, 1, number_of_gammas)
    
    N = 100
    
    T = np.linspace(0, 10, N)
    check_points = [1,5,8]
    
    Timepoint_of_interest=0
    Z_initial = 1
    N1 = 10
    N2 = 10
    sigma = 0.1
    #sigma_yp = 0.1
    noise_level_s = 0.1
    noise_level_j = 0.3
    noise_level = 0.2
    c1=1e-4
    c2=0.9
    eta_init=1
    tol=1e-6
    maga = 0.3
    gamma_init = 0.5
    gamma_true = 0.3
    
    
    
    observed_y, observed_yp = simulate_observed_data(gamma_true, k, initial_conditions, (T[0], T[-1]), N, noise_level_s, noise_level_j)
    #print(f"data is  : observed_y = {observed_y}, observed_yp = {observed_yp}  ")
    #for t in range(1, len(T)):
        #log_likelihood_values = [] 
       # h = T[t] - T[t - 1]
       #observed_y_t = observed_y[t]
        #y0 = [observed_y[t - 1], 0] 
       # for gamma in gammas:
            
          #  y_simulated = euler_forward(gamma, k, y0, t, h)
           # y0 = [observed_y[t - 1], y_simulated[1]] 
          #  ll_value = log_likelihood(y_simulated, observed_y[t], noise_level)
           # log_likelihood_values.append(ll_value)

        # Convert to numpy array for easier plotting
       # log_likelihood_values = np.array(log_likelihood_values)

        # Plotting the log-likelihood function
       # plt.figure(figsize=(10, 6))
      #  plt.plot(gammas, log_likelihood_values, label=f'Time step {t}')
       # plt.xlabel('Gamma')
       # plt.title(f'Log-Likelihood vs. Gamma at Time step {t}')
       # plt.legend()
       # plt.grid(True)
      #  plt.show()
    
    
    p_e, p_t , P_E, P_T ,Z_E,Z_T,Z_e,Z_t = find_POST_t(gammas,gamma_init,observed_y, observed_yp,noise_level, N, k, initial_conditions, T,  tol, eta_init, c1, c2, M=2)
    print("Z_e=",Z_e)  
    print("Z_t=",Z_t)  
    print("Z_E=",Z_E)  
    print("Z_T=",Z_T)  
    #print("p_e = ",p_e)
    #print("p_t = ",p_t)
    
    
    checkpoints = list(range(N))

    plt.figure(figsize=(12, 6))

    # Plotting data
    plt.plot(checkpoints, observed_y, label='y', marker='o', linestyle='-', color='blue')
    plt.plot(checkpoints, observed_yp, label='yp', marker='x', linestyle='--', color='green')
    
    plt.xlabel('Checkpoints')
    plt.ylabel('Values')
    plt.title('Data of y and yp across Checkpoints')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    
    
    
    
    plt.figure(figsize=(10, 6))
    plt.plot(range(len(p_e)), p_e, label='p_e', marker='o', linestyle='-', color='blue')
    plt.plot(range(len(p_t)), p_t, label='p_t', marker='o', linestyle='-', color='red')
    plt.xlabel('Time Step')
    plt.ylabel('Probability')
    plt.title('Comparison of p_e and p_t Over Time Steps')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    
    


    plt.figure(figsize=(14, 8))

    plt.plot(range(len(Z_e)), Z_e, label='Z_e', marker='o', linestyle='-', color='blue', alpha=0.5)
    plt.plot(range(len(Z_E)), Z_E, label='Z_E', marker='x', linestyle='--', color='blue', alpha=0.8)
    

    plt.plot(range(len(Z_t)), Z_t, label='Z_t', marker='o', linestyle='-', color='red', alpha=0.5)
    plt.plot(range(len(Z_T)), Z_T, label='Z_T', marker='x', linestyle='--', color='red', alpha=0.8)

    for i in range(len(Z_e)):
        if Z_e[i] > Z_E[i]:
            plt.text(i, Z_e[i], 'Z_e', fontsize=9, ha='center', color='blue')
        else:
            plt.text(i, Z_E[i], 'Z_E', fontsize=9, ha='center', color='blue')

    for i in range(len(Z_t)):
        if Z_t[i] > Z_T[i]:
            plt.text(i, Z_t[i], 'Z_t', fontsize=9, ha='center', color='red')
        else:
            plt.text(i, Z_T[i], 'Z_T', fontsize=9, ha='center', color='red')

    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.title('Comparison of Z_e, Z_E, Z_t, and Z_T Over Time Steps')
    plt.legend()
    plt.grid(True)
    plt.show()
      

    

    agreement_status = []
    for i in range(len(p_e)):
        if (p_e[i] > p_t[i] and P_E[i] > P_T[i]) or (p_t[i] > p_e[i] and P_T[i] > P_E[i]):
            agreement_status.append(1)  
        else:
            agreement_status.append(0)

    plt.figure(figsize=(10, 6))
    for i, status in enumerate(agreement_status):
        color = 'green' if status == 1 else 'red'
        plt.scatter(i, 1, color=color)  # Place the dot on y=1 for simplicity

    plt.xlabel('Time Step')
    plt.ylabel('Agreement')
    plt.title('Agreement between p_e, p_t and P_E,P_T at Each Time Step')
    plt.grid(True)
    plt.show()