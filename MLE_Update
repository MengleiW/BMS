import numpy as np
import scipy
import matplotlib.pyplot as plt
from scipy.optimize import minimize
import torch
from scipy.misc import derivative
import numdifftools as nd



def damped_oscillator(t, y, gamma, k):
    """
    Models a damped oscillator system.

    Inputs:
          t: Float, the current time point.
          y: List, current state of the system.
          gamma: Float, damping coefficient.
          k: Float, stiffness coefficient.

    Outputs:
         [y[1], f_t - gamma*y[1] - k*y[0]]: List, Derivative of the system state.
    """
    f_t = 0  
    #f_t = np.sin(t) 
    gamma = gamma if isinstance(gamma, float) else gamma[0]
    #print("gamma=",[y[1], f_t - gamma*y[1] - k*y[0]])
    return [y[1], f_t - gamma*y[1] - k*y[0]]

def simulate_observed_data(gamma, k, initial_conditions, ts, N, noise_level_s, noise_level_j):
    """
    Simulates the damped oscillator system over a given time span using solve_ivp.

    Inputs:
      gamma: Float, damping coefficient.
      k: Float, stiffness coefficient.
      initial_conditions: List, initial state of the system.
      ts: List, time span for simulation .
      N: Integer, number of points to evaluate in the time span.
      smooth_noise_level: number, level of random noise in the first half (smooth phase).
      jumpy_noise_level: number, level of random noise in the second half (jumpy phase).
    Outputs:
      (y, yp): list, where y is the position array and yp is the velocity array over the time span.
    """
    
    sol = scipy.integrate.solve_ivp(damped_oscillator, ts, initial_conditions, args=(gamma, k), t_eval=T)
    
    y = sol.y[0]
    yp = sol.y[1]
    #print("y1=",y)
    half = N // 2
    # Apply smooth noise to the first half and jumpy noise to the second half
    noise1 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    
    noise2 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    y += noise1
    #print("noise1=",noise1)
    yp += noise2
    #print("y2=",y)
    return y, yp

def euler_forward(gamma, k, y0, t, h):
    """
    Numerically approximates the solution of the damped oscillator using the Euler forward method for a single time point.

    Inputs:
      gamma: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      y0: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """

    y_t_plus_one = y0 + h * np.array(damped_oscillator(t, y0, gamma, k))
    print(f"Euler Method: gamma={gamma}, y0={y0}, y_t_plus_one={y_t_plus_one}")  # Debugging statement
    return y_t_plus_one

def trapezoidal_method(gamma, k, y0, t, h ):
    """
    Numerically approximates the solution of the damped oscillator using the trapezoidal method for a single time point.

    Inputs:
      gammas: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      initial_conditions: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    f_n = np.array(damped_oscillator(t, y0, gamma, k))
    y_pred = y0 + h * f_n
    f_n_plus_1 = np.array(damped_oscillator(t + h, y_pred, gamma, k))
    y_t_puls_one = y0 + h / 2 * (f_n + f_n_plus_1)
    return y_t_puls_one



def log_likelihood(simulated, observed_y,  noise_level):
    """
    Computes the combined log-likelihood of the observed position data for a given simulated dataset.
  
    Inputs:
        simulated: Array, the simulated data from the model.
        observed_y: Array, observed data for position (y).
        observed_yp: Array, observed data for velocity (y').
        noise_level: Float, standard deviation of noise for the data.


    Outputs:
        log_likelihood: Float, the combined log-likelihood value of the observed data given the simulated model outputs.

    """
    
    residuals = observed_y - simulated[0]
    #print("residuals=", residuals)
    #print("observed_y=", observed_y) 
    #print(" simulated[0]=",  simulated[0])
    ll = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level ** 2) - 0.5 / (noise_level ** 2) * (residuals ** 2)
    #print("Log-likelihood", ll)  
    return ll




def gradient_log_likelihood_euler(gamma, observed_y, noise_level, k, y0,t, h):
    """
    Computes the gradient of the log-likelihood with respect to the damping coefficient gamma using the Euler forward method.

    Inputs:
        observed_y: Array of observed position data.
        gamma: Float, the current value of the damping coefficient.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float,times step length.
        noise_level: Float, standard deviation of noise for the data.
        
        Z_t: Normalizing constant.

    Outputs:
        grad_ll: Float, the gradient of the log-likelihood with respect to gamma.
    """
    y = euler_forward(gamma, k, y0, t, h)
    #grad_ll = np.gradient(y, gamma)
    ll_gamma = log_likelihood(y, observed_y, noise_level)
    y_plus_h = euler_forward(gamma + h, k, y0, t, h)
    ll_gamma_h = log_likelihood(y_plus_h, observed_y, noise_level)
    grad_ll = (ll_gamma_h - ll_gamma) / h
    
    return grad_ll




def gradient_log_likelihood_trapezoidal(gamma, observed_y, noise_level, k, y0, t, h):
    """
   Computes the gradient of the log-likelihood with respect to the damping coefficient gamma using the trapezoidal method.

   Inputs:
       observed_y: Array of observed position data.
       gamma: Float, the current value of the damping coefficient.
       k: Float, stiffness coefficient.
       y0: List, initial conditions of the system.
       t: Float, current time.
       h: Float,times step length.
       noise_level: Float, standard deviation of noise for the data.
       Z_t: Normalizing constant.

   Outputs:
       grad_ll: Float, the gradient of the log-likelihood with respect to gamma.
   """
    
    y = trapezoidal_method(gamma, k, y0, t, h)
    ll_gamma = log_likelihood(y, observed_y, noise_level)
    y_plus_h = trapezoidal_method(gamma + h, k, y0, t, h)
    ll_gamma_h = log_likelihood(y_plus_h, observed_y, noise_level)
    grad_ll = (ll_gamma_h - ll_gamma) / h
    return grad_ll


def gradient_log_likelihood_euler_pt(gamma, observed_y, noise_level, k, y0, T, Z):
    gamma_torch = torch.tensor(gamma, requires_grad=True, dtype=torch.float32)
    observed_y_torch = torch.tensor(observed_y, dtype=torch.float32)
    noise_level_torch = torch.tensor(noise_level, dtype=torch.float32)

    predicted_y = euler_forward(gamma_torch, k, y0, T)
    predicted_y = torch.tensor(predicted_y[:, 0], dtype=torch.float32)
    
    loss = log_likelihood(predicted_y, observed_y_torch, noise_level_torch)
    loss.backward()
    
    gradient = gamma_torch.grad.item()
    
    return gradient


def gradient_log_likelihood_trapezoidal_pt(gamma, observed_y, noise_level, k, y0, T, Z):
    gamma_torch = torch.tensor(gamma, requires_grad=True, dtype=torch.float32)
    observed_y_torch = torch.tensor(observed_y, dtype=torch.float32)
    noise_level_torch = torch.tensor(noise_level, dtype=torch.float32)

    predicted_y = trapezoidal_method(gamma_torch, k, y0, T)
    predicted_y = torch.tensor(predicted_y[:, 0], dtype=torch.float32)
    
    loss = log_likelihood(predicted_y, observed_y_torch, noise_level_torch)
    loss.backward()
    
    gradient = gamma_torch.grad.item()
    
    return gradient






def gradient_descent(log_likelihood, grad_log_likelihood, gamma_init, observed_y, noise_level, k, y0, t, h, Z_t, tol, eta_init, c1, c2, method):
    """
    Performs gradient descent using the Wolfe line search to find an optimal value for the damping coefficient gamma.

    Inputs:
        log_likelihood: Function to compute the log-likelihood.
        grad_log_likelihood: Function to compute the gradient of the log-likelihood.
        gamma_init: Float, initial guess for gamma which is the posterior of last iteration.
        observed_y: Array of observed position data.
        noise_level: Float, standard deviation of noise for the data.
        N: Integer, number of time points.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float,times step length.
        Z_t: Normalizing constant.
        tol: Float, tolerance for convergence.
        eta_init: Float, initial step size.
        c1: Float, Armijo condition constant.
        c2: Float, curvature condition constant.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        gamma: Float, the optimal value of the damping coefficient.
    """
    gamma = gamma_init
    
    maximum_iteration = 1000
    
    
    for i in range(maximum_iteration):
        gradient = grad_log_likelihood(gamma, observed_y, noise_level, k, y0, t, h, Z_t)
        if np.isnan(gradient):
            print(f"Gradient contains NaN values at iteration {i}: {gradient}")
            break
        
        print("gradient=",gradient)
        
        eta = eta_init
        while True:
            gamma_new = gamma - eta * gradient
            print("gamma_new",gamma_new)
            print("eta",eta)
            #sipu.optimaize
            #np.clip does the job of if the vaule is over the max or min, it will replace the it with the closest one.
            #gamma_new = np.clip(gamma_new, 0, 1)
            # Wolfe conditions: Armijo rule and curvature condition
            armijo_condition = log_likelihood(method(gamma_new, k, y0, t, h), observed_y, noise_level) <= log_likelihood(method(gamma, k, y0, t, h), observed_y, noise_level) + c1 * eta * gradient **2
            curvature_condition = grad_log_likelihood(gamma_new, observed_y, noise_level, k, y0, t, h, Z_t) * (-gradient) >= c2 * gradient **2
            
            # Adjust eta based on Wolfe conditions
            if armijo_condition and curvature_condition:
                break
            
            eta *= 0.5
        gamma = gamma_new
        
        # Check for convergence
        if np.abs(gradient) < tol:
            break    
    return gamma

def optimize_gamma(log_likelihood, grad_log_likelihood, gamma_init, observed_y, noise_level, k, y0, t, h, method):
    result = scipy.optimize.minimize(
        fun=lambda gamma: -log_likelihood(method(gamma, k, y0, t, h), observed_y, noise_level),
        x0=gamma_init,
        jac=lambda gamma: -grad_log_likelihood(gamma, observed_y, noise_level, k, y0, t, h),
        bounds=[(0, 1)],
        method='L-BFGS-B'
    )
    #print(f"Optimization result: {result}")  # Debugging statement
    return result.x
def central_difference_second_derivative(f, x, h=1e-5):
    """
    Estimate the second derivative of a function using the central difference method.
    Inputs:
        f: Function for which to compute the second derivative.
        x: Point at which to compute the second derivative.
        h: Step size for the central difference method.
    Outputs:
        second_derivative: Estimated second derivative of the function at x.
    """
    f_x_plus_h = f(x + h)
    f_x_minus_h = f(x - h)
    f_x = f(x)
    second_derivative = (f_x_plus_h - 2 * f_x + f_x_minus_h) / (h ** 2)
    
    print(f"f_x_plus_h: {f_x_plus_h}, f_x: {f_x}, f_x_minus_h: {f_x_minus_h}, second_derivative: {second_derivative}")
    
    return second_derivative
    
    print(f"f_x_plus_h: {f_x_plus_h}, f_x: {f_x}, f_x_minus_h: {f_x_minus_h}, second_derivative: {second_derivative}")
    
    return second_derivative
def fisher_information(log_likelihood, gamma, observed_y, noise_level, k, y0, t, h, method):
    """
    Calculates the Fisher information using numerical differentiation.

    Inputs:
        log_likelihood: Function to compute the log-likelihood.
        gamma: Float, the current value of the damping coefficient.
        observed_y: Array of observed position data.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        fisher_info: Float, the Fisher information value.
    """
    
    
    def ll(y_gamma):
        y_gamma = method(gamma, k, y0, t, h)
        ll_value = log_likelihood(y_gamma, observed_y, noise_level)
        print(f"y_gamma: {y_gamma}, ll_value: {ll_value}") 
        return ll_value
    #second_derivative = derivative(ll, gamma, dx=1e-5, n=2)
    #Compute the second derivative using numdifftools
    #hessian = nd.Hessian(ll, step=1e-5)
    #second_derivative = hessian(gamma)[0, 0]
    
    
    second_derivative = central_difference_second_derivative(ll, gamma)
    print(f"second_derivative: {second_derivative}")
    
 
    if np.isnan(second_derivative) or second_derivative >= 0:
        return np.inf

    

def find_POST_t(gamma_init, observed_y, noise_level, N, k, y0, T, tol, eta_init, c1, c2, M=2):
    """
   Calculates the normalizing constant Z_t(M) after finding the MLE gamma.

   Inputs:
       gamma_init: Float, initial guess for gamma.
       observed_y: Array of observed position data.
       noise_level: Float, standard deviation of noise for the data.
       N: Integer, number of time points.
       k: Float, stiffness coefficient.
       y0: List, initial conditions of the system.
       T: Array, time points for the simulation.
       tol: Float, tolerance for convergence.
       eta_init: Float, initial step size.
       c1: Float, Armijo condition constant.
       c2: Float, curvature condition constant.
       M: Integer, number of methods (default is 2).

   Outputs:
       P_e: Float, the updated normalizing constant for Euler method.
       P_t: Float, the updated normalizing constant for trapezoidal method.
   """
    Z_e = [0.5]
    Z_t = [0.5]
    MLE_e = gamma_init
    MLE_t = gamma_init
    P_e = [0.5]
    P_t = [0.5]
    
    number_of_gammas = 10
    
    for t in range(1, len(T)):
        print("ttime step=",t)
        h = T[t] - T[t - 1]
        observed_y_t = observed_y[t]
        y0 = [observed_y[t - 1], 0] 
                
        MLE_e = optimize_gamma(log_likelihood, gradient_log_likelihood_euler, P_e[-1], observed_y_t, noise_level, k, y0, T[t - 1], h, euler_forward)
        MLE_t = optimize_gamma(log_likelihood, gradient_log_likelihood_trapezoidal, P_t[-1], observed_y_t, noise_level, k, y0, T[t - 1], h, trapezoidal_method)
        print("MLE_e = ", MLE_e)
        print("MLE_t = ", MLE_t)

       

        fisher_e = fisher_information(log_likelihood, MLE_e, observed_y_t, noise_level, k, y0, T[t - 1], h, euler_forward)
        fisher_t = fisher_information(log_likelihood, MLE_t, observed_y_t, noise_level, k, y0, T[t - 1], h, trapezoidal_method)
        print("fisher_e = ", fisher_e)
        print("fisher_t = ", fisher_t)
        if np.any(fisher_e <= 0) or np.any(fisher_t <= 0):
            break
        posterior_e = scipy.stats.norm(loc=MLE_e, scale=np.sqrt(1 / fisher_e))
        posterior_t = scipy.stats.norm(loc=MLE_t, scale=np.sqrt(1 / fisher_t))
        print("posterior_e=", posterior_e)

        model_prior_e = (P_e[-1])
        model_prior_t = (P_t[-1])
        print("posterior_e = ", posterior_e)
        print("posterior_t = ", posterior_t)

        likelihood_e = np.exp(log_likelihood(euler_forward(MLE_e, k, y0, T[t - 1], h), observed_y_t, noise_level))
        likelihood_t = np.exp(log_likelihood(trapezoidal_method(MLE_t, k, y0, T[t - 1], h), observed_y_t, noise_level))

        print("likelihood_e = ", likelihood_e)
        print("likelihood_t = ", likelihood_t)
        if np.any(np.isnan(likelihood_e)) or np.any(np.isnan(likelihood_t)):
            print("NaN in likelihoods")
            break

        Z_e.append((likelihood_e * 1 / number_of_gammas) / posterior_e.pdf(MLE_e))
        Z_t.append((likelihood_t * 1 / number_of_gammas) / posterior_t.pdf(MLE_t))

        if np.any(np.isnan(Z_e[-1])) or np.any(np.isnan(Z_t[-1])):
            print("NaN in Z_t or Z_e")
            break

        P_e.append(model_prior_e * Z_e[-1] / (model_prior_e * Z_e[-1] + model_prior_t * Z_t[-1]))
        P_t.append(model_prior_t * Z_t[-1] / (model_prior_e * Z_e[-1] + model_prior_t * Z_t[-1]))

    return P_e, P_t

  

if __name__ == '__main__':   
    
    #print('debug=',combined_log_likelihood)
    # Parameters 
    m = 1
    k = 0.5
    initial_conditions = [1, 0]
    number_of_gammas = 10
    Dimention_of_parameter_space = 1
    gammas = np.linspace(0, 1, number_of_gammas)
    
    N = 200
    
    T = np.linspace(0, 10, N)
    check_points = [1,5,8]
    
    Timepoint_of_interest=0
    Z_initial = 1
    N1 = 10
    N2 = 10
    sigma = 0.1
    #sigma_yp = 0.1
    noise_level_s = 0.4
    noise_level_j = 0.5
    noise_level = 0.4
    c1=1e-4
    c2=0.9
    eta_init=1
    tol=1e-6
    maga = 0.3
    gamma_init = 0.5
    gamma_true = 0.3
    
    
    
    observed_y, observed_yp = simulate_observed_data(gamma_true, k, initial_conditions, (T[0], T[-1]), N, noise_level_s, noise_level_j)

    p_e, p_t = find_POST_t(gamma_init,observed_y, noise_level, N, k, initial_conditions, T,  tol, eta_init, c1, c2, M=2)
    
    print("p_e = ",p_e)
    print("p_t = ",p_t)

    


      
    
