import numpy as np
import scipy
import matplotlib.pyplot as plt
from scipy.optimize import minimize
import torch
from scipy.misc import derivative
import numdifftools as nd
import time
from scipy.integrate import quadrature
from matplotlib.ticker import MaxNLocator
from numpy.linalg import inv
from scipy.integrate import quad

def damped_oscillator(t, y, gamma, k):
    """
    Models a damped oscillator system.

    Inputs:
          t: Float, the current time point.
          y: List, current state of the system.
          gamma: Float, damping coefficient.
          k: Float, stiffness coefficient.

    Outputs:
         [y[1], f_t - gamma*y[1] - k*y[0]]: List, Derivative of the system state.
    """
    f_t = 0  
    #f_t = np.sin(t)
    #print("gamma=",gamma)
    gamma = gamma if isinstance(gamma, float) else gamma[0]
    
    return [y[1], f_t - gamma*y[1] - k*y[0]]

def simulate_observed_data(gamma, k, initial_conditions, ts, N, noise_level_s, noise_level_j):
    """
    Simulates the damped oscillator system over a given time span using solve_ivp.

    Inputs:
      gamma: Float, damping coefficient.
      k: Float, stiffness coefficient.
      initial_conditions: List, initial state of the system.
      ts: List, time span for simulation .
      N: Integer, number of points to evaluate in the time span.
      smooth_noise_level: number, level of random noise in the first half (smooth phase).
      jumpy_noise_level: number, level of random noise in the second half (jumpy phase).
    Outputs:
      (y, yp): list, where y is the position array and yp is the velocity array over the time span.
    """
    t_eval = np.linspace(ts[0], ts[1], N)
    sol = scipy.integrate.solve_ivp(damped_oscillator, ts, initial_conditions, args=(gamma, k), t_eval=t_eval)
    
    y = sol.y[0]
    yp = sol.y[1]
    #print("y1=",y)
    half = N // 2
    # Apply smooth noise to the first half and jumpy noise to the second half
    noise1 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    
    noise2 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    y = y+ noise1
    #print("noise1=",noise1)
    yp = yp+ noise2
    #print("y2=",y)
    return y, yp,sol

def euler_forwarda(gamma, k, y0, t, h):
    """
    Numerically approximates the solution of the damped oscillator using the Euler forward method for a single time point.

    Inputs:
      gamma: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      y0: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    #print("gamma=",gamma)
    y_t_plus_one = y0 + h * np.array(damped_oscillator(t, y0, gamma, k))
    #print(f"Euler Method: gamma={h}, y0={damped_oscillator(t, y0, gamma, k)}, y_t_plus_one={y_t_plus_one}") 
    return y_t_plus_one

def trapezoidal_methoda(gamma, k, y0, t, h ):
    """
    Numerically approximates the solution of the damped oscillator using the trapezoidal method for a single time point.

    Inputs:
      gammas: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      initial_conditions: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    f_n = np.array(damped_oscillator(t, y0, gamma, k))
    y_pred = y0 + h * f_n
    f_n_plus_1 = np.array(damped_oscillator(t + h, y_pred, gamma, k))
    y_t_puls_one = y0 + h / 2 * (f_n + f_n_plus_1)
    return y_t_puls_one

def euler_forward(gamma, k, y0, T):
    """
   Numerically approximates the solution of the damped oscillator using the Euler forward method.

   Inputs:
     gamma: Float, damping coefficient.
     k: Float, stiffness coefficient.
     y0: 1D Array, initial state of the system (position and velocity).
     T: Array, time points.

   Outputs:
     y: 2D Array, where each row corresponds to the system states (position and velocity) at each time point.
   """
    y = np.zeros((len(T), 2))
    y[0, :] = y0
    
    if len(T)>1:
        for i, t in enumerate(T[:-1]):
            h = T[i+1] - T[i]
            y[i + 1, :] = y[i, :] + h * np.array(damped_oscillator(t, y[i, :], gamma, k))
            
    return y[-1, :]

def trapezoidal_method(gamma, k, y0, T):
    """
    Numerically approximates the solution of the damped oscillator using the trapezoidal method for a single time point.

    Inputs:
      gammas: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      initial_conditions: 2D Array, initial states of the system for each gamma (position and velocity).
      T: Float, Time points.
      

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    y = np.zeros((len(T), 2))
    y[0, :] = y0
    if len(T)>1:
        for i, t in enumerate(T[:-1]):
            h = T[i+1] - T[i]
            f_n = np.array(damped_oscillator(t, y[i, :], gamma, k))
            y_pred = y[i, :] + h * f_n
            f_n_plus_1 = np.array(damped_oscillator(t + h, y_pred, gamma, k))
            y[i + 1, :] = y[i, :] + h / 2 * (f_n + f_n_plus_1)

    return y[-1, :]

def log_likelihood(simulated, observed_y,observed_yp,  noise_level):
    """
    Computes the combined log-likelihood of the observed position data for a given simulated dataset.
  
    Inputs:
        simulated: Array, the simulated data from the model.
        observed_y: Array, observed data for position (y).
        observed_yp: Array, observed data for velocity (y').
        noise_level: Float, standard deviation of noise for the data.


    Outputs:
        log_likelihood: Float, the combined log-likelihood value of the observed data given the simulated model outputs.

    """
    
    residuals_y = observed_y - simulated[0]  
    residuals_yp = observed_yp - simulated[1]  
    
    #print("residuals_yp=", residuals_yp)
    #print("residuals_y=", residuals_y) 
    #print(" simulated[0]=",  simulated)
    ll_y = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level ** 2) - 0.5 / (noise_level ** 2) * (residuals_y ** 2)
    ll_yp = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level ** 2) - 0.5 / (noise_level ** 2) * (residuals_yp ** 2)
    #print("Log-likelihood", ll_y + ll_yp)  
    return ll_y + ll_yp





def gradient_log_likelihood_euler(gamma, observed_y,observed_yp, noise_level, k, y0,T, h):
    """
    Computes the gradient of the log-likelihood with respect to the damping coefficient gamma using the Euler forward method.

    Inputs:
        observed_y: Array of observed position data.
        gamma: Float, the current value of the damping coefficient.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float,times step length.
        noise_level: Float, standard deviation of noise for the data.
        
        Z_t: Normalizing constant.

    Outputs:
        grad_ll: Float, the gradient of the log-likelihood with respect to gamma.
    """
    y = euler_forward(gamma, k, y0, T)
    #grad_ll = np.gradient(y, gamma)
    ll_gamma = log_likelihood(y, observed_y,observed_yp, noise_level)
    y_plus_h = euler_forward(gamma + h, k, y0, T)
    ll_gamma_h = log_likelihood(y_plus_h, observed_y,observed_yp, noise_level)
    grad_ll = (ll_gamma_h - ll_gamma) / h
    
    return grad_ll




def gradient_log_likelihood_trapezoidal(gamma, observed_y,observed_yp, noise_level, k, y0, T, h):
    """
   Computes the gradient of the log-likelihood with respect to the damping coefficient gamma using the trapezoidal method.

   Inputs:
       observed_y: Array of observed position data.
       gamma: Float, the current value of the damping coefficient.
       k: Float, stiffness coefficient.
       y0: List, initial conditions of the system.
       t: Float, current time.
       h: Float,times step length.
       noise_level: Float, standard deviation of noise for the data.
       Z_t: Normalizing constant.

   Outputs:
       grad_ll: Float, the gradient of the log-likelihood with respect to gamma.
   """
    
    y = trapezoidal_method(gamma, k, y0, T)
    
    ll_gamma = log_likelihood(y, observed_y,observed_yp, noise_level)
    y_plus_h = trapezoidal_method(gamma + h, k, y0, T)
    ll_gamma_h = log_likelihood(y_plus_h, observed_y,observed_yp, noise_level)
    grad_ll = (ll_gamma_h - ll_gamma) / h
    return grad_ll










def optimize_gamma(log_likelihood, grad_log_likelihood, gamma_init, observed_y, observed_yp,noise_level, k, y0, T, h, method):
    
    result = scipy.optimize.minimize(
        fun=lambda gamma: -log_likelihood(method(gamma, k, y0, T), observed_y,observed_yp, noise_level),
        x0=gamma_init,
        jac=lambda gamma: -grad_log_likelihood(gamma, observed_y, observed_yp,noise_level, k, y0, T,h),
        bounds=[(0, 1)],
        method='L-BFGS-B'
    )
    #print(f"Optimization result: {result}")  # Debugging statement
    
    return result.x

    
    




def fisher_information(log_likelihood, gamma, observed_y,observed_yp, noise_level, k, y0, T, h, method):
    """
    Calculates the Fisher information using numerical differentiation.

    Inputs:
        log_likelihood: Function to compute the log-likelihood.
        gamma: Float, the current value of the damping coefficient.
        observed_y: Array of observed position data.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        fisher_info: Float, the Fisher information value.
    """
    
    
    epsilon = 0.0000001
    ll_gamma = log_likelihood(method(gamma, k, y0, T), observed_y,observed_yp, noise_level)
    #print("ll_gamma =",ll_gamma)
    
    def neg_log_likelihood(gamma_value):
        
        simulated = method(gamma_value, k, y0, T)
        ll = log_likelihood(simulated, observed_y, observed_yp, noise_level)
        return -ll  

    # Compute the Hessian (second derivative)
    hess = nd.Hessian(neg_log_likelihood)(gamma)[0, 0]
    fisher = max(abs(hess), 1e-12)      # always positive & non-zero
    return fisher
    
    
    
    
    
    
    return fisher


    

def find_POST_t(gammas,gamma_init, observed_y,observed_yp, noise_level, N, k, initial_conditions, T, tol, eta_init, c1, c2, M=2):
    """
   Calculates the normalizing constant Z_t(M) after finding the MLE gamma.

   Inputs:
       gamma_init: Float, initial guess for gamma.
       observed_y: Array of observed position data.
       noise_level: Float, standard deviation of noise for the data.
       N: Integer, number of time points.
       k: Float, stiffness coefficient.
       y0: List, initial conditions of the system.
       T: Array, time points for the simulation.
       tol: Float, tolerance for convergence.
       eta_init: Float, initial step size.
       c1: Float, Armijo condition constant.
       c2: Float, curvature condition constant.
       M: Integer, number of methods (default is 2).

   Outputs:
       P_e: Float, the updated normalizing constant for Euler method.
       P_t: Float, the updated normalizing constant for trapezoidal method.
   """
    Z_e = [0.5]
    Z_t = [0.5]
    MLE_e = gamma_init
    MLE_t = gamma_init
    P_e = [0.5]
    P_t = [0.5]
    
    Z_E =[1]
    Z_T = [1]
    P_E = [0.5]
    P_T = [0.5]
    number_of_gammas = 10
    y0 = initial_conditions
    num_internal_steps = 10
    
    
    
    for t in range(2, len(T)):
        #print("len(T)=",len(T))
        #print("time step=",t)
        h = T[t] - T[t - 2]
        
        observed_y_t = observed_y[t]
        observed_yp_t = observed_yp[t]
        
        y0 = [observed_y[t - 2], observed_yp[t - 2]] 
        
        T_internal = np.linspace(T[t - 2], T[t], num_internal_steps*2 + 1)
        #print("T_internal = ",T_internal)
        #print("y00=",y0)        
        MLE_e = optimize_gamma(log_likelihood, gradient_log_likelihood_euler, P_e[-1], observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, euler_forward)
        MLE_t = optimize_gamma(log_likelihood, gradient_log_likelihood_trapezoidal, P_t[-1], observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, trapezoidal_method)
        #print("MLE_e = ", MLE_e)
        #print("MLE_t = ", MLE_t)
        
       

        neg_fisher_e = fisher_information(log_likelihood, MLE_e, observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, euler_forward)
        neg_fisher_t = fisher_information(log_likelihood, MLE_t, observed_y_t,observed_yp_t, noise_level, k, y0, T_internal, h, trapezoidal_method)
        print("one_over_neg_Fisher_e = ", neg_fisher_e)
        print("one_over_neg_Fisher_t = ", neg_fisher_t)
        neg_fisher_e = max(abs(neg_fisher_e), 1e-12)
        neg_fisher_t = max(abs(neg_fisher_t), 1e-12)
        posterior_e = scipy.stats.norm(loc=MLE_e, scale=np.sqrt(1 / neg_fisher_e))
        posterior_t = scipy.stats.norm(loc=MLE_t, scale=np.sqrt(1 / neg_fisher_t))
        #print("posterior_e=", posterior_e)
        #print("posterior_t = ", posterior_t)
        
        model_prior_e = (P_e[-1])
        model_prior_t = (P_t[-1])
        
        

        likelihood_e = np.exp(log_likelihood(euler_forward(MLE_e, k, y0, T_internal), observed_y_t,observed_yp_t, noise_level))
        likelihood_t = np.exp(log_likelihood(trapezoidal_method(MLE_t, k, y0, T_internal), observed_y_t, observed_yp_t,noise_level))

        #print("likelihood_e = ", likelihood_e)
        #print("likelihood_t = ", likelihood_t)
        if np.any(np.isnan(likelihood_e)) or np.any(np.isnan(likelihood_t)):
            print("NaN in likelihoods")
            break
        
        Z_e.append(((likelihood_e ) / posterior_e.pdf(MLE_e))[0])
        Z_t.append(((likelihood_t ) / posterior_t.pdf(MLE_t))[0])
        #print("Z_e=",Z_e)
        #print("Z_t=",Z_t)
        if np.any(np.isnan(Z_e[-1])) or np.any(np.isnan(Z_t[-1])):
            print("NaN in Z_t or Z_e")
            break
        P_e.append(model_prior_e * Z_e[-1] / (model_prior_e * Z_e[-1] + model_prior_t * Z_t[-1]))
        P_t.append(model_prior_t * Z_t[-1] / (model_prior_e * Z_e[-1] + model_prior_t * Z_t[-1]))
        #print("p_e = ",P_e)
        
        
        Z_E_bayesian_euler = compute_Z_quadrature(observed_y[t], observed_yp[t], noise_level, k, y0, T_internal, h, euler_forward)
        #Z_E_bayesian_euler_check = compute_Z_quadrature_E(observed_y[t], observed_yp[t], noise_level, k, y0, T_internal, h)
        #print(f"Large Z-value at time step {t}: Z_E_bayesian_euler = {Z_E_bayesian_euler},  Z_E_bayesian_euler_check  = { Z_E_bayesian_euler_check }  ")
        #if np.any(Z_E_bayesian_euler != Z_E_bayesian_euler_check) :
            #print("something wrong with quadratrue")
            #break
        
        Z_T_bayesian_trapezoidal = compute_Z_quadrature(observed_y[t], observed_yp[t], noise_level, k, y0, T_internal, h, trapezoidal_method)

        Z_E.append(Z_E_bayesian_euler)
        Z_T.append(Z_T_bayesian_trapezoidal)
        
        
        #P_E.append([(np.exp(-50*((observed_y_t-y0[0]-h*y0[1])**2+(observed_yp_t-y0[1]+k*h*y0[0]-gamma*h*y0[1])**2)))  / Z_E_bayesian_euler_check for gamma in gammas])
        P_E.append([np.exp(log_likelihood(euler_forward(gamma, k, y0, T_internal), observed_y_t, observed_yp_t, noise_level))  / Z_E_bayesian_euler for gamma in gammas])
        P_T.append([np.exp(log_likelihood(trapezoidal_method(gamma, k, y0, T_internal), observed_y_t, observed_yp_t, noise_level))  / Z_T_bayesian_trapezoidal for gamma in gammas])

        #print("Z_T=",Z_T)   
        
        threshold = 1000
        if Z_e[-1] > threshold or Z_t[-1] > threshold or t == 1 or t==5 or t ==8:
            
        
            #print(f"Large Z-value at time step {t}: Z_E_bayesian_euler = {Z_E_bayesian_euler}, Z_E_bayesian_euler_check = {Z_E_bayesian_euler_check}  ")
            print(f"Large ZE-value at time step {t}: Z_e = {Z_e[t-2]} ")
            print(f"Large observed_y-value at time step {t}: observed_y_t = {observed_y_t}, observed_yp_t= {observed_yp_t}  ")
            print(f"Large y-values at time step {t}: y = {y0[0]}, yp = {y0[1]}  ")
            #print(f"Posterior_e = {posterior_e.pdf(MLE_e[0])}, Posterior_t = {posterior_t.pdf(MLE_t[0])} ")
            #print(f" Likelihood_e = {likelihood_e} , Likelihood_t = {likelihood_t}")
            #print(f" one_over_neg_fisher_e = {neg_fisher_e} , one_over_neg_fisher_t = {neg_fisher_t}")
            #print(f"MLE_E = {MLE_e}   MLE_T={MLE_t}" )
            y_ee = euler_forward(MLE_e, k, y0, T_internal)
            y_tt = trapezoidal_method(MLE_t, k, y0, T_internal)
            residuals_y_e = observed_y[t] - y_ee[0]  
            residuals_yp_e = observed_yp[t] - y_ee[1] 
            #print(f" residuals_y_e = {residuals_y_e} , residuals_yp_e = {residuals_yp_e}")
            residuals_y_t = observed_y[t] - y_tt[0]  
            residuals_yp_t = observed_yp[t] - y_tt[1] 
            #print(f" residuals_y_t = {residuals_y_t} , residuals_yp_t = {residuals_yp_t}")
            
            posterior_e_vals = posterior_e.pdf(gammas)
           
            
            #print(f"posterior_e_vals = {posterior_e_vals}")
                
                
            posterior_e_bayesian_vals = P_E[-1]
            #print(f"posterior_e_bayesian_vals = {posterior_e_bayesian_vals} " )
            
            plt.figure(figsize=(8, 6))
            plt.plot(gammas, posterior_e_vals, label=f'Euler Posterior at t={t}', color='blue')
            
            plt.title(f'Posterior Distributions at Time Step {t}')
            plt.xlabel('Gamma')
            plt.ylabel('Posterior (Likelihood Normalized)')
            plt.legend()
            plt.grid(True)
            plt.show()
            
            
            
            plt.figure(figsize=(8, 6))
            plt.plot(gammas, posterior_e_bayesian_vals, label=f'Bayesian Euler Posterior at t={t}', color='green',marker='o', linestyle='--')
            plt.title(f'Posterior Distributions at Time Step {t}')
            plt.xlabel('Gamma')
            plt.ylabel('Posterior (Likelihood Normalized)')
            plt.legend()
            plt.grid(True)
            plt.show()
    return P_e, P_t , P_E, P_T ,Z_E,Z_T,Z_e,Z_t

  

def compute_Z_quadrature1(observed_y_t, observed_yp_t,noise_level, k, y0, T, h, method):
    """
    Calculate the normalization constant Z using quadrature integration.

    Inputs:
        observed_y_t: Array, observed data for position (y) at time t.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        Z: Float, the computed normalization constant using quadrature integration.
    """

    integrand = lambda gamma: np.exp(log_likelihood(method(gamma, k, y0, T), observed_y_t, observed_yp_t, noise_level))

    Z, _ = quadrature(integrand, 0, 1, tol=1e-8, maxiter=10000)
    return Z

def compute_Z_quadrature_E(observed_y_t, observed_yp_t,noise_level, k, y0, T, h):
    """
    Calculate the normalization constant Z using quadrature integration.

    Inputs:
        observed_y_t: Array, observed data for position (y) at time t.
        noise_level: Float, standard deviation of noise for the data.
        k: Float, stiffness coefficient.
        y0: List, initial conditions of the system.
        t: Float, current time.
        h: Float, time step length.
        method: Numerical method for simulation (e.g., euler_forward or trapezoidal_method).

    Outputs:
        Z: Float, the computed normalization constant using quadrature integration.
    """

    integrand = lambda gamma: np.exp((-0.5/(sigma)**2)*((observed_y_t-y0[0]-h*y0[1])**2+(observed_yp_t-y0[1]+k*h*y0[0]-gamma*h*y0[1])**2))
    
    #Z = np.exp(-50*((observed_y_t-y0[0]-h*y0[1])**2+(observed_yp_t-y0[1]+k*h*y0[0])**2))-0.5*y0[1]*(observed_yp_t-y0[1]+k*h*y0[0])+h**2*1/3*(y0[1])**2
    
    
    Z, _ = quadrature(integrand, 0, 1, tol=1e-8, maxiter=10000)
    return Z

def abc_smc(method, theta_prior, y_obs, ydot_obs, sigma_t, t_grid, y0,
            num_particles, num_generations, rng=None):
    """
    Returns (thetas, weights, Z_values) for each generation.
    Now Z_values[g] = approximate ABC marginal likelihood.
    """
    if rng is None:
        rng = np.random.default_rng(0)

    thetas   = np.empty((num_generations, num_particles))
    weights  = np.empty_like(thetas)
    Z_values = np.zeros(num_generations)

    prev_distances = None
    w_prev = None

    for g in range(num_generations):
        # ─── initialize ABC counters ─────────────────────
        accept_count = 0
        total_count  = 0

        if g == 0:
            # 1) draw from prior
            pool = rng.choice(theta_prior, size=num_particles, replace=True)
            w    = np.full(num_particles, 1/num_particles)

            accept_count = num_particles
            total_count  = num_particles

            distances = [compute_distance( simulate_model(method, th, k, y0, t_grid),y_obs, ydot_obs, sigma_t)
                for th in pool
            ]
            eps = np.percentile(distances, 20)

        else:
            eps = np.percentile(distances, 20)

            pool = []
            trials = 0
            while len(pool) < num_particles:
                trials += 1
                idx  = rng.choice(num_particles, p=w_prev)
                cand = thetas[g-1, idx] + rng.normal(0, 0.12)
                if cand < 0 or cand > 1:
                    continue

                total_count += 1
                sim = simulate_model(method, cand, k, y0, t_grid)
                dist = compute_distance(sim, y_obs, ydot_obs, sigma_t)

                if dist <= eps:
                    pool.append(cand)
                    accept_count += 1

                elif total_count > 100_000:
                    needed = num_particles - len(pool)
                    extras = rng.choice(theta_prior, size=needed, replace=True)
                    pool.extend(extras.tolist())
                    break

            pool = np.array(pool)

            distances = [compute_distance(simulate_model(method, th, k, y0, t_grid),y_obs, ydot_obs, sigma_t )
                for th in pool
            ]

            # importance weight correction
            kern = scipy.stats.norm.pdf( pool[None, :],loc=thetas[g-1, :, None],scale=0.12)
            denom = np.dot(w_prev, kern)
            w = 1.0 / (denom * len(theta_prior))
            w /= w.sum()

        # ─── store results ──────────────────────────────
        thetas[g]  = pool
        weights[g] = w

        # ABC marginal likelihood estimate
        if total_count > 0:
            Z_values[g] = accept_count / total_count * np.mean(w)
        else:
            Z_values[g] = 0.0

        prev_distances = distances
        w_prev = w.copy()

        ESS = 1.0 / np.sum(w**2)
        print(f"gen {g}: eps={eps:.3f}, ESS={ESS:.1f}, Z={Z_values[g]:.3e}")

    return thetas, weights, Z_values

def euler_series(gamma, k, y0, t_grid):
    y = np.empty((2, len(t_grid)))
    y[:, 0] = y0
    for i in range(len(t_grid) - 1):
        h = t_grid[i+1] - t_grid[i]
        y[:, i+1] = y[:, i] + h * np.array(
            damped_oscillator(t_grid[i], y[:, i], gamma, k))
    return y


def trapezoidal_series(gamma, k, y0, t_grid):
    y = np.empty((2, len(t_grid)))
    y[:, 0] = y0
    for i in range(len(t_grid) - 1):
        h   = t_grid[i+1] - t_grid[i]
        f_n = np.array(damped_oscillator(t_grid[i], y[:, i], gamma, k))
        y_pred = y[:, i] + h * f_n
        f_np1  = np.array(damped_oscillator(t_grid[i+1], y_pred, gamma, k))
        y[:, i+1] = y[:, i] + 0.5*h*(f_n + f_np1)
    return y




def simulate_model(method, theta, k_val, y0_val, t_grid):
    """
    Run the solver (Euler or trapezoidal) for a full trajectory.
    Returns a (2, len(t_grid)) array.
    """
    return method(theta, k_val, y0_val, t_grid)

def compute_distance(sim, y_obs, ydot_obs, sigma_t):
    y_sim, ydot_sim = sim
    dy = (y_sim - y_obs) / sigma_t
    dyp = (ydot_sim - ydot_obs) / sigma_t
    return np.sqrt(np.mean(dy**2 + 3 * dyp**2)) 



from scipy.integrate import quad

def compute_Z_integral(method, y_obs, ydot_obs, sigma_t, k_val, y0_val, t_grid):
    """
    Numerically integrate over θ∈[0,1]:
      Z = ∫ p(y_obs|θ) dθ
    using a log-sum-exp trick to avoid underflow.
    """
    # Step 1: estimate the maximum log-likelihood across theta grid
    theta_grid = np.linspace(0, 1, 101)
    log_likelihoods = []
    for theta in theta_grid:
        sim = method(theta, k_val, y0_val, t_grid)
        ll = np.sum(log_likelihood(sim, y_obs, ydot_obs, sigma_t))
        log_likelihoods.append(ll)
    max_ll = np.max(log_likelihoods)

    # Step 2: define underflow-safe integrand
    def integrand(theta):
        sim = method(theta, k_val, y0_val, t_grid)
        ll = np.sum(log_likelihood(sim, y_obs, ydot_obs, sigma_t))
        return np.exp(ll - max_ll)

    # Step 3: integrate and rescale
    Z_raw, _ = quad(integrand, 0.0, 1.0, epsabs=1e-6)
    Z = Z_raw * np.exp(max_ll)
    return Z



# ─── EKF‐within‐window ───────────────────────────────────────────────────
def ekf_window(thetas, weights, y_obs, t_grid,
               g_func, h_func, Q, R, x0, P0, λ=0.99):
    Np = len(thetas)
    dim = len(x0)
    Xp = np.zeros((Np,dim))
    Pp = np.zeros((Np,dim,dim))

    for i, θ in enumerate(thetas):
        x_i, P_i = x0.copy(), P0.copy()
        for j in range(len(t_grid)-1):
            dt = t_grid[j+1]-t_grid[j]
            # predict
            x_pred, G = g_func(x_i, θ, dt)
            P_pred = (G@P_i@G.T)/λ + Q
            # update
            y_meas = y_obs[j+1]
            h_pred, H = h_func(x_pred)
            S = H@P_pred@H.T + R
            K = P_pred@H.T @ inv(S)
            x_i = x_pred + K@(y_meas - h_pred)
            P_i = (np.eye(dim)-K@H)@P_pred

        Xp[i] = x_i
        Pp[i] = P_i

    # combine
    x_mean = np.average(Xp, axis=0, weights=weights)
    cov = np.zeros_like(P0)
    for i in range(Np):
        dx = (Xp[i]-x_mean)[:,None]
        cov += weights[i]*(Pp[i] + dx@dx.T)
    return x_mean, cov

# ─── dynamics & observation Jacobians ──────────────────────────────────
def g_with_jacobian(x, θ, dt):
    # continuous F matrix
    F = np.array([[0,1],[-k/m, -θ/m]])
    G = np.eye(2) + F*dt
    x_next = G@x
    return x_next, G

def h_and_jacobian(x):
    # measure posistionm & velosity directly
    H = np.eye(2)
    return x, H






from scipy.special import logsumexp

def compute_Z_grid(method, y_obs, ydot_obs, sigma_t,
                   k_val, y0_val, t_grid, theta_prior):
    """
    Approximate Z = ∫₀¹ p(y|θ) dθ
    on the discrete theta_prior by
      logZ ≈ logsumexp( log p(y|θ_j) ) - log(N)
    (since ∆θ = 1/N for uniform grid).
    """
    # 1) compute log‐likelihoods on the same window:
    logLs = []
    for θ in theta_prior:
        sim = method(θ, k_val, y0_val, t_grid)      # (2, len(t_grid))
        # sum over both position & velocity
        ll  = np.sum(log_likelihood(sim, y_obs, ydot_obs, sigma_t))
        logLs.append(ll)
    logLs = np.array(logLs)

    # 2) use logsumexp to get logZ
    #    grid has N points in [0,1] so ∆θ = 1/N
    N = len(theta_prior)
    logZ = logsumexp(logLs) - np.log(N)

    # 3) back to normal space
    return np.exp(logZ)
    

def compute_Z_quadrature(observed_y_t, observed_yp_t, noise_level,
                         k, y0, T, h, method):
    """
    Calculate the normalization constant Z using quadrature integration.
    """
    integrand = lambda gamma: np.exp(
        np.sum(   # <— sum over the whole window
          log_likelihood(method(gamma, k, y0, T),observed_y_t, observed_yp_t,noise_level )))




if __name__ == '__main__':   
   

    
    #print('debug=',combined_log_likelihood)
    # Parameters 
    m = 1
    k = 5
    initial_conditions = [1, 0]
    number_of_gammas = 10
    Dimention_of_parameter_space = 1
    gammas = np.linspace(0, 1, number_of_gammas)
    
    N = 1000
    
    T = np.linspace(0, 10, N)
    check_points = [1,5,8]
    
    Timepoint_of_interest=0
    Z_initial = 1
    
    sigma = 0.1
    #sigma_yp = 0.1
    noise_level_s = 0.005
    noise_level_j = 0.005
    noise_level = 0.05
    c1=1e-4
    c2=0.9
    eta_init=1
    tol=1e-6
    maga = 0.3
    gamma_init = 0.5
    gamma_true = 0.3
    
    P0 = np.eye(2)*1e2   # large prior cov
    Q  = np.eye(2)*1e-6  # tiny process noise
    R  = np.eye(2)*0.1**2
    
    observed_y, observed_yp,sol = simulate_observed_data(gamma_true, k, initial_conditions, (T[0], T[-1]), N, noise_level_s, noise_level_j)
        
    W_min = 20     # min size of time window 
    W_max = 50       # max size of time window 
    deltas= 10 # step by which W grows/shrinks
    W_boot= 30        # first two bootstrap windows
    err_hist = []        # store eps_r values to compare
    
    
    P_e, P_t = [0.5], [0.5]             # model posteriors
    Z_e, Z_t = [1.], [1.]               # evidence trackers
    W = W_boot                       # current window size
    t_idx= 2                                # first index with a full W
    
    theta_prior = np.linspace(0, 1, 101)
    Z_e_hist, Z_t_hist = [], []
    Z_e_int_hist, Z_t_int_hist = [], []
    P_e_hist, P_t_hist = [0.5], [0.5]
    X_e_hist, X_t_hist = [], []
    Z_e_err_hist   = []
    err_hist = []
    Z_t_err_hist   = []
    x_e_err_hist   = []
    x_t_err_hist   = []
    
    y0_e = np.array(initial_conditions, dtype=float)
    y0_t = np.array(initial_conditions, dtype=float)   
    W = W_boot
    num_particles=100;num_generations=10

    for t_idx in [W_boot - 1, 2 * W_boot - 1]:
        idx0, idx1 = t_idx - W + 1, t_idx + 1
        t_slice    = T[idx0:idx1]
        y_slice    = observed_y[idx0:idx1]
        yp_slice   = observed_yp[idx0:idx1]
        sigma_sl   = np.where(t_slice <= t_slice[-1]/2,
                              noise_level_s, noise_level_j)

        # — make y0 available to abc_smc via the global name y0 —
        y0 = y0_e
        thetaE, wE, Z_e_vals = abc_smc( euler_series, theta_prior,y_slice, yp_slice,
                                       sigma_sl, t_slice, y0_e, num_particles,num_generations )
        Z_e_hat = np.prod(Z_e_vals)
        Z_e_hist.append(Z_e_hat)

        # — EKF update for Euler, carry forward —
        x_e, P_e = ekf_window(
            thetas=thetaE[-1], weights=wE[-1],
            y_obs=y_slice, t_grid=t_slice,
            g_func=lambda x, θ, dt: ( x + dt * np.array(damped_oscillator(None, x, θ, k)),  np.array([[1, dt], [-k*dt, 1-θ*dt]]) ),
            h_func=lambda x: (x, np.eye(2)), Q=Q, R=R, x0=y0_e, P0=P0 )
        X_e_hist.append(x_e)
        y0_e = x_e  # next window’s starting state

        # — now do trapezoidal with its own y0 —
        y0 = y0_t
        thetaT, wT, Z_t_vals = abc_smc(trapezoidal_series,theta_prior, y_slice, yp_slice, sigma_sl,
            t_slice,y0_t, num_particles,num_generations)
        Z_t_hat = np.prod(Z_t_vals)
        Z_t_hist.append(Z_t_hat)

        x_t, P_t = ekf_window(
            thetas=thetaT[-1], weights=wT[-1],
            y_obs=y_slice, t_grid=t_slice,
            g_func=lambda x, θ, dt: ( x + dt * np.array(damped_oscillator(None, x, θ, k)),np.array([[1, dt], [-k*dt, 1-θ*dt]]) ),
            h_func=lambda x: (x, np.eye(2)),Q=Q, R=R, x0=y0_t, P0=P0 )
        X_t_hist.append(x_t)
        y0_t = x_t

        # — direct integral (diagnostic) —
        Ze_int = compute_Z_integral( euler_series, y_slice, yp_slice, sigma_sl, k, y0_e, t_slice )
        Zt_int = compute_Z_integral( trapezoidal_series,  y_slice, yp_slice, sigma_sl,  k, y0_t, t_slice)
        Z_e_int_hist.append(Ze_int)
        Z_t_int_hist.append(Zt_int)

        # — update model posteriors (ABC only) —
        Pe, Pt = P_e_hist[-1], P_t_hist[-1]
        Pe_next = Pe * Z_e_hat / (Pe*Z_e_hat + Pt*Z_t_hat)
        P_e_hist.append(Pe_next)
        P_t_hist.append(1.0 - Pe_next)

        # — Mahalanobis‐style error (for window adapt) —
        res  = np.array([y_slice[-1] - y0_e[0], yp_slice[-1] - y0_e[1]])
        epsr = np.sqrt(res.T @ inv(P_e + R) @ res)
        err_hist.append(epsr)

    # ─── Sliding/adaptive windows until end ──────────────────────
    t_idx = 2*W_boot - 1
    while True:
        t_idx = min(N-1, t_idx + W)
        idx0, idx1 = max(0, t_idx - W + 1), t_idx + 1
        t_slice    = T[idx0:idx1]
        y_slice    = observed_y[idx0:idx1]
        yp_slice   = observed_yp[idx0:idx1]
        sigma_sl   = np.where(t_slice <= t_slice[-1]/2,
                              noise_level_s, noise_level_j)

        # — Euler ABC & EKF, using y0_e —
        y0 = y0_e
        thetaE, wE, Z_e_vals = abc_smc(euler_series, theta_prior, y_slice, yp_slice,sigma_sl, t_slice,y0_e,
            num_particles, num_generations)
        Z_e_hat = np.prod(Z_e_vals); Z_e_hist.append(Z_e_hat)
        x_e, P_e = ekf_window(
            thetas=thetaE[-1], weights=wE[-1],
            y_obs=y_slice, t_grid=t_slice,
            g_func=lambda x, θ, dt: (  x + dt * np.array(damped_oscillator(None, x, θ, k)),np.array([[1, dt], [-k*dt, 1-θ*dt]])),
            h_func=lambda x: (x, np.eye(2)),Q=Q, R=R, x0=y0_e, P0=P0 )
        X_e_hist.append(x_e); y0_e = x_e

        # — Trapezoid ABC & EKF, using y0_t —
        y0 = y0_t
        thetaT, wT, Z_t_vals = abc_smc( trapezoidal_series, theta_prior,y_slice, yp_slice,sigma_sl, t_slice,y0_t,
            num_particles, num_generations )
        Z_t_hat = np.prod(Z_t_vals); Z_t_hist.append(Z_t_hat)
        x_t, P_t = ekf_window(thetas=thetaT[-1], weights=wT[-1],y_obs=y_slice, t_grid=t_slice,
                   g_func=lambda x, θ, dt: (x + dt * np.array(damped_oscillator(None, x, θ, k)),np.array([[1, dt], [-k*dt, 1-θ*dt]])),
            h_func=lambda x: (x, np.eye(2)),
            Q=Q, R=R, x0=y0_t, P0=P0)
        X_t_hist.append(x_t); y0_t = x_t

        # — direct integral again for diagnostics —
        Ze_int = compute_Z_integral(euler_series, y_slice, yp_slice, sigma_sl,  k, y0_e, t_slice )
        Zt_int = compute_Z_integral( trapezoidal_series, y_slice, yp_slice, sigma_sl,k, y0_t, t_slice)
        Z_e_int_hist.append(Ze_int)
        Z_t_int_hist.append(Zt_int)

        # — model posterior update —
        Pe, Pt = P_e_hist[-1], P_t_hist[-1]
        Pe_next = Pe * Z_e_hat / (Pe*Z_e_hat + Pt*Z_t_hat)
        P_e_hist.append(Pe_next)
        P_t_hist.append(1.0 - Pe_next)

        # — error & adapt window —
        res  = np.array([y_slice[-1] - y0_e[0],
                         yp_slice[-1] - y0_e[1]])
        epsr = np.sqrt(res.T @ inv(P_e + R) @ res)
        err_hist.append(epsr)
        if len(err_hist) > 1:
            W = min(W+deltas, W_max) if err_hist[-2] < err_hist[-1] \
                else max(W-deltas, W_min)

        if t_idx == N-1:
            break

    # ─── Final plots ─────────────────────────────────────────────
    x = np.arange(len(Z_e_hist))

    # 1) Euler: ABC vs direct‐integral
    plt.figure(figsize=(8,4))
    plt.plot(x, Z_e_hist,    'o-', label='Z_e (ABC)')
    plt.plot(x, Z_e_int_hist,'s--',label='Z_e (integral)')
    plt.yscale('log')
    plt.title('Euler evidence: ABC vs integral')
    plt.legend(); plt.grid(which='both', alpha=0.3); plt.show()

    # 2) Trapezoid: ABC vs direct‐integral
    plt.figure(figsize=(8,4))
    plt.plot(x, Z_t_hist,    'o-', label='Z_t (ABC)')
    plt.plot(x, Z_t_int_hist,'s--',label='Z_t (integral)')
    plt.yscale('log')
    plt.title('Trapezoid evidence: ABC vs integral')
    plt.legend(); plt.grid(which='both', alpha=0.3); plt.show()

    # 3) Bayes factor from ABC only
    BF_abc = np.array(Z_t_hist) / np.array(Z_e_hist)
    plt.figure(figsize=(8,4))
    plt.plot(BF_abc, 'o-', label='BF = Z_t/Z_e')
    plt.axhline(1.0, color='k', lw=1)
    plt.title('Bayes factor (ABC only)')
    plt.legend(); plt.grid(alpha=0.3); plt.show()
    
    #p_e, p_t , P_E, P_T ,Z_E,Z_T,Z_e,Z_t = find_POST_t(gammas,gamma_init,observed_y, observed_yp,noise_level, N, k, initial_conditions, T,  tol, eta_init, c1, c2, M=2)
    #print("Z_e=",Z_e)  
    #print("Z_t=",Z_t)  
    #print("Z_E=",Z_E)  
    #print("Z_T=",Z_T)  
    #print("p_e = ",p_e)
    #print("p_t = ",p_t)
    
    
    plt.figure(figsize=(10, 6))
    plt.plot(T, sol.y[0], label='True Solution (Position)', color='blue')
    plt.scatter(T, observed_y, label='Observed Data (Position)', color='red', marker='o')
    plt.title('Damped Oscillator Position')
    plt.xlabel('Time')
    plt.ylabel('Position (y)')
    plt.legend()
    plt.grid(True)
    plt.show()
            
    # Plot for velocity y'
    plt.figure(figsize=(10, 6))
    plt.plot(T, sol.y[1], label='True Solution (Velocity)', color='blue')
    plt.scatter(T, observed_yp, label="Observed Data (Velocity)", color='red', marker='o')
    plt.title("Damped Oscillator Velocity")
    plt.xlabel('Time')
    plt.ylabel("Velocity (y')")
    plt.legend()
    plt.grid(True)
    plt.show()
    
    
    
    Lp = min(len(P_e_hist), len(P_t_hist))
    if Lp > 0:
        x = np.arange(1, Lp+1)
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(x, P_e_hist[:Lp],
                marker='o', linestyle='-',  color='blue',  label=r'$P_e$ (Euler)')
        ax.plot(x, P_t_hist[:Lp],
                marker='s', linestyle='--', color='red',   label=r'$P_t$ (Trap)')
        ax.set_xlabel('Window index')
        ax.set_ylabel('Model posterior')
        ax.set_title('Evolution of model‐posterior $P_e$ vs.\ $P_t$')
        ax.set_ylim(-0.05, 1.05)
        # only show up to 10 ticks
        step = max(1, Lp // 10)
        ax.set_xticks(x[::step])
        ax.set_xticklabels(x[::step], rotation=45, ha='right')
        ax.xaxis.set_major_locator(MaxNLocator(integer=True))
        ax.grid(alpha=0.3)
        fig.tight_layout()
        plt.show()
    else:
        print("No posterior history to plot.")
    
    
    # ——— 2) Improved Evidence plot —————————————————————————————————
    Lz = len(Z_e_hist)
    if Lz > 0:
        x = np.arange(1, Lz+1)
    
        fig, ax = plt.subplots(figsize=(12, 5))
    
        # 1) raw scatter
        ax.scatter(x, Z_e_hist,
                   color='C0', alpha=0.4, s=30,
                   label=r'$Z_e$ (Euler)')
        ax.scatter(x, Z_t_hist,
                   color='C1', alpha=0.4, s=30,
                   label=r'$Z_t$ (Trap)')
    
        # 2) moving average smooth
        window = 5
        if Lz >= window:
            kernel = np.ones(window)/window
            Ze_smooth = np.convolve(Z_e_hist, kernel, mode='valid')
            Zt_smooth = np.convolve(Z_t_hist, kernel, mode='valid')
            xs = x[(window-1)//2 : -(window//2)]
            ax.plot(xs, Ze_smooth, color='C0', lw=2, label='Z$_e$ (smoothed)')
            ax.plot(xs, Zt_smooth, color='C1', lw=2, label='Z$_t$ (smoothed)')
    
        # layout
        ax.set_xlabel('Window index')
        ax.set_ylabel('Evidence $Z$')
        ax.set_title('End-of-Window Evidence: Raw & 5-window Moving Average')
        ax.set_yscale('linear')              # linear for 0.1–0.5 range
        # at most 10 ticks
        step = max(1, Lz//10)
        ax.set_xticks(x[::step])
        ax.set_xticklabels(x[::step], rotation=45, ha='right')
        ax.xaxis.set_major_locator(MaxNLocator(integer=True))
        ax.grid(alpha=0.3)
        ax.legend(loc='upper right')
        fig.tight_layout()
        plt.show()
    else:
        print("No evidence history to plot.")
    

    

