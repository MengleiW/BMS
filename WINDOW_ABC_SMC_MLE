import numpy as np
import scipy
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.integrate import quadrature, quad
from matplotlib.ticker import MaxNLocator
from numpy.linalg import inv
from scipy.special import logsumexp

# ============== Core Functions ==============

def damped_oscillator(t, y, gamma, k):
    """
    Models a damped oscillator system.

    Inputs:
          t: Float, the current time point.
          y: List, current state of the system.
          gamma: Float, damping coefficient.
          k: Float, stiffness coefficient.

    Outputs:
         [y[1], f_t - gamma*y[1] - k*y[0]]: List, Derivative of the system state.
    """
    f_t = 0  
    #f_t = np.sin(t)
    #print("gamma=",gamma)
    gamma = gamma if isinstance(gamma, float) else gamma[0]
    
    return [y[1], f_t - gamma*y[1] - k*y[0]]

def simulate_observed_data(gamma, k, initial_conditions, ts, N, noise_level_s, noise_level_j):
    """
    Simulates the damped oscillator system over a given time span using solve_ivp.

    Inputs:
      gamma: Float, damping coefficient.
      k: Float, stiffness coefficient.
      initial_conditions: List, initial state of the system.
      ts: List, time span for simulation .
      N: Integer, number of points to evaluate in the time span.
      smooth_noise_level: number, level of random noise in the first half (smooth phase).
      jumpy_noise_level: number, level of random noise in the second half (jumpy phase).
    Outputs:
      (y, yp): list, where y is the position array and yp is the velocity array over the time span.
    """
    t_eval = np.linspace(ts[0], ts[1], N)
    sol = scipy.integrate.solve_ivp(damped_oscillator, ts, initial_conditions, args=(gamma, k), t_eval=t_eval)
    
    y = sol.y[0]
    yp = sol.y[1]
    #print("y1=",y)
    half = N // 2
    # Apply smooth noise to the first half and jumpy noise to the second half
    noise1 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    
    noise2 = np.concatenate([np.random.normal(0, noise_level_s,  half),
                             np.random.normal(0, noise_level_j, N -  half)])
    y = y+ noise1
    #print("noise1=",noise1)
    yp = yp+ noise2
    #print("y2=",y)
    return y, yp,sol


def euler_series(gamma, k, y0, t_grid):
    """
    Numerically approximates the solution of the damped oscillator using the Euler forward method for a single time point.

    Inputs:
      gamma: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      y0: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    y = np.empty((2, len(t_grid)))
    y[:, 0] = y0
    for i in range(len(t_grid) - 1):
        h = t_grid[i+1] - t_grid[i]
        dy = np.array(damped_oscillator(t_grid[i], y[:, i], gamma, k))
        # Check for extreme values
        if np.any(np.abs(dy) > 1e6):
            # Cap extreme derivatives
            dy = np.clip(dy, -1e6, 1e6)
        y[:, i+1] = y[:, i] + h * dy
        # Check for instability
        if np.any(np.isnan(y[:, i+1])) or np.any(np.abs(y[:, i+1]) > 1e10):
            # Fill rest with NaN to indicate failure
            y[:, i+1:] = np.nan
            break
    return y

def trapezoidal_series(gamma, k, y0, t_grid):
    """
    Numerically approximates the solution of the damped oscillator using the trapezoidal method for a single time point.

    Inputs:
      gammas: Array, range of damping coefficients.
      k: Float, stiffness coefficient.
      initial_conditions: 2D Array, initial states of the system for each gamma (position and velocity).
      t: Float, current time.
      h: Float,times step length.

    Outputs:
      y: 2D Array, where each row corresponds to the system states (position and velocity) for a specific gamma.
    """
    y = np.empty((2, len(t_grid)))
    y[:, 0] = y0
    for i in range(len(t_grid) - 1):
        h = t_grid[i+1] - t_grid[i]
        f_n = np.array(damped_oscillator(t_grid[i], y[:, i], gamma, k))
        # Check for extreme values
        if np.any(np.abs(f_n) > 1e6):
            f_n = np.clip(f_n, -1e6, 1e6)
        y_pred = y[:, i] + h * f_n
        f_np1 = np.array(damped_oscillator(t_grid[i+1], y_pred, gamma, k))
        if np.any(np.abs(f_np1) > 1e6):
            f_np1 = np.clip(f_np1, -1e6, 1e6)
        y[:, i+1] = y[:, i] + 0.5*h*(f_n + f_np1)
        # Check for instability
        if np.any(np.isnan(y[:, i+1])) or np.any(np.abs(y[:, i+1]) > 1e10):
            # Fill rest with NaN to indicate failure
            y[:, i+1:] = np.nan
            break
    return y

def log_likelihood_point(y_sim, yp_sim, y_obs, yp_obs, noise_level):
    """
    Computes the combined log-likelihood of the observed position data for a given simulated dataset.
  
    Inputs:
        y_sim: Array, the simulated data from the model.
        yp_sim: Array, the simulated data from the model.
        y_obs: Array, observed data for position (y).
        yp_obs: Array, observed data for velocity (y').
        noise_level: Float, standard deviation of noise for the data.


    Outputs:
        log_likelihood: Float, the combined log-likelihood value of the observed data given the simulated model outputs.

    """
    residual_y = y_obs - y_sim
    residual_yp = yp_obs - yp_sim
    
    ll_y = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level**2) - \
           0.5 / (noise_level**2) * (residual_y**2)
    ll_yp = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(noise_level**2) - \
            0.5 / (noise_level**2) * (residual_yp**2)
    
    return ll_y + ll_yp

def log_likelihood_trajectory(sim_trajectory, y_obs, yp_obs, noise_levels):
    """
    Computes the total log-likelihood of observing data given a simulated trajectory.

    Inputs:
      sim_trajectory: 2×N array, Simulated states over N time points.
      y_obs: 1D array of length N, Observed positions at each time point.
      yp_obs: 1D array of length N, Observed velocities at each time point.
      noise_levels: Float or 1D array of length N, Observation noise standard deviation(s).

    Outputs:
      ll_total: Float, Sum of pointwise log-likelihoods over the trajectory.
    """
    # Check for NaN or inf
    if np.any(np.isnan(sim_trajectory)) or np.any(np.isinf(sim_trajectory)):
        return -np.inf
    
    if np.isscalar(noise_levels):
        noise_levels = np.full(len(y_obs), noise_levels)
    
    ll_total = 0.0
    for t in range(len(y_obs)):
        ll_total += log_likelihood_point(
            sim_trajectory[0, t], sim_trajectory[1, t],
            y_obs[t], yp_obs[t], noise_levels[t]
        )
    return ll_total

def compute_distance(sim, y_obs, ydot_obs, sigma_t):
    """
    Compute distance between simulated and observed trajectories.


    sim : ndarray, shape (2, N), Simulated trajectory [position; velocity].
    y_obs : array_like, shape (N,), Observed positions.
    ydot_obs : array_like, shape (N,), Observed velocities.
    sigma_t : float or array_like, Observation noise standard deviation(s).

    Returns
    distance: float,  Normalized distance metric.
    """
    if np.any(np.isnan(sim)) or np.any(np.isinf(sim)):
        return 1e10
    
    y_sim, ydot_sim = sim[0, :], sim[1, :]
    sigma = sigma_t[0] if hasattr(sigma_t, '__len__') else sigma_t
    
    res_y = (y_sim - y_obs) / sigma
    res_yp = (ydot_sim - ydot_obs) / sigma
    
    dist_y = np.sqrt(np.percentile(res_y**2, 75))
    dist_yp = np.sqrt(np.percentile(res_yp**2, 75))
    
    return np.sqrt(dist_y**2 + dist_yp**2)

def abc_smc_improved(method, theta_prior, y_obs, ydot_obs, sigma_t, t_grid, y0_val,
                     num_particles, num_generations, k_val, theta_min, theta_max, 
                     window_idx=0, rng=None):
    """
    Improved ABC-SMC algorithm with adaptive tolerance scheduling.

   
    method : callable, Integration method (euler_series or trapezoidal_series).
    theta_prior : array_like, Prior parameter values.
    y_obs : array_like, Observed positions.
    ydot_obs : array_like
        Observed velocities.
    sigma_t : float or array_like
        Observation noise standard deviation(s).
    t_grid : array_like
        Time points.
    y0_val : array_like, shape (2,)
        Initial conditions.
    num_particles : int
        Number of particles in each generation.
    num_generations : int
        Number of ABC-SMC generations.
    k_val : float
        Stiffness coefficient.
    theta_min : float
        Minimum parameter value.
    theta_max : float
        Maximum parameter value.
    window_idx : int, optional
        Window index for debug output control.
    rng : RandomState, optional
        Random number generator.

    Returns
    -------
    tuple
        (thetas, weights, Z_estimate) where thetas are parameter values,
        weights are importance weights, and Z_estimate is evidence estimate.
    """
    if rng is None:
        rng = np.random.default_rng(0)

    dim = 1
    thetas = np.empty((num_generations, num_particles))
    weights = np.empty_like(thetas)
    log_likelihoods = []
    
    percentiles = np.linspace(95, 50, num_generations)
    prev_distances = None

    for g in range(num_generations):
        if g == 0:
            pool = rng.choice(theta_prior, size=num_particles, replace=True)
            w = np.full(num_particles, 1/num_particles)
            
            distances = []
            valid_pool = []
            for th in pool:
                sim = method(th, k_val, y0_val, t_grid)
                dist = compute_distance(sim, y_obs, ydot_obs, sigma_t)
                if dist < 1000:
                    distances.append(dist)
                    valid_pool.append(th)
            
            pool = np.array(valid_pool)
            
            if len(pool) < num_particles:
                additional = rng.choice(pool, size=num_particles-len(pool), replace=True)
                pool = np.concatenate([pool, additional])
                for th in additional:
                    sim = method(th, k_val, y0_val, t_grid)
                    dist = compute_distance(sim, y_obs, ydot_obs, sigma_t)
                    distances.append(dist)
            
            eps = np.percentile(distances, percentiles[g])
            prev_distances = distances

        else:
            eps = np.percentile(prev_distances, percentiles[g])
            pool = []
            new_distances = []
            attempts = 0
            
            param_range = theta_max - theta_min
            proposal_std = param_range * 0.1 * (1 - g/num_generations) + param_range * 0.01
            
            while len(pool) < num_particles:
                attempts += 1
                
                idx = rng.choice(num_particles, p=weights[g-1])
                cand = thetas[g-1, idx] + rng.normal(0, proposal_std)
                
                if cand < theta_min or cand > theta_max:
                    continue
                
                sim = method(cand, k_val, y0_val, t_grid)
                dist = compute_distance(sim, y_obs, ydot_obs, sigma_t)
                
                if dist > 1000:
                    continue
                
                if dist <= eps:
                    pool.append(cand)
                    new_distances.append(dist)
                
                if attempts > num_particles * 20:
                    if len(pool) < num_particles / 2 and g == num_generations - 1:
                        print(f"    Note: Partial convergence ({len(pool)}/{num_particles} particles)")
                    remaining = num_particles - len(pool)
                    if remaining > 0:
                        best_idx = np.argsort(prev_distances)[:remaining]
                        for idx in best_idx:
                            pool.append(thetas[g-1, idx])
                            new_distances.append(prev_distances[idx])
                    break
            
            pool = np.array(pool)
            distances = new_distances
            prev_distances = distances
            
            kern = scipy.stats.norm.pdf(
                pool[None, :],
                loc=thetas[g-1, :, None],
                scale=proposal_std
            )
            denom = np.dot(weights[g-1], kern) + 1e-10
            w = 1.0 / denom
            w /= w.sum()

        thetas[g] = pool
        weights[g] = w
        
        median_dist = np.median(distances)
        if median_dist > 0:
            kde_bw = max(0.1 * np.std(distances), 0.01)
            log_likes = [-0.5 * (d/kde_bw)**2 for d in distances]
            log_likelihood_est = logsumexp(log_likes) - np.log(len(distances))
        else:
            log_likelihood_est = 0.0
        
        log_likelihoods.append(log_likelihood_est)
        
        ESS = 1.0 / np.sum(w**2)
        median_dist = np.median(distances) if len(distances) > 0 else 0
        if g == num_generations - 1 and window_idx < 2:
            print(f"    Final: eps={eps:.3f}, ESS={ESS:.1f}, median_dist={median_dist:.3f}")

    # Return particles, weights, and placeholder for evidence
    return thetas, weights, 666

def ekf_window(thetas, weights, y_obs, yp_obs, t_grid, k_val,
               Q, R, x0, P0, λ=0.99):
    """
    EKF within window for ensemble of particles.


    thetas : array_like, Parameter values for each particle.
    weights : array_like,Importance weights for each particle.
    y_obs : array_like,  Observed positions.
    yp_obs : array_like,Observed velocities.
    t_grid : array_like,  Time points.
    k_val : float, Stiffness coefficient.
    Q : ndarray, shape (2, 2),Process noise covariance.
    R : ndarray, shape (2, 2), Measurement noise covariance.
    x0 : array_like, shape (2,), Initial state estimate.
    P0 : ndarray, shape (2, 2), Initial covariance estimate.
    λ : float, optional, Forgetting factor.

    Returns

    tuple, (x_mean, cov) where x_mean is the weighted mean state estimate
        and cov is the weighted covariance.
    """
    Np = len(thetas)
    dim = len(x0)
    Xp = np.zeros((Np, dim))
    Pp = np.zeros((Np, dim, dim))

    for i, θ in enumerate(thetas):
        x_i, P_i = x0.copy(), P0.copy()
        
        try:
            for j in range(len(t_grid)-1):
                dt = t_grid[j+1] - t_grid[j]
                
                F = np.array([[0, 1], [-k_val, -θ]])
                G = np.eye(2) + F * dt
                x_pred = G @ x_i
                P_pred = (G @ P_i @ G.T) / λ + Q
                
                y_meas = np.array([y_obs[j+1], yp_obs[j+1]])
                H = np.eye(2)
                
                S = H @ P_pred @ H.T + R
                K = P_pred @ H.T @ inv(S)
                x_i = x_pred + K @ (y_meas - H @ x_pred)
                P_i = (np.eye(dim) - K @ H) @ P_pred
                
                if np.any(np.isnan(x_i)) or np.any(np.isinf(x_i)):
                    x_i = y_meas
                    P_i = R
        except:
            x_i = np.array([y_obs[-1], yp_obs[-1]])
            P_i = R
        
        Xp[i] = x_i
        Pp[i] = P_i

    x_mean = np.average(Xp, axis=0, weights=weights)
    cov = np.zeros_like(P0)
    for i in range(Np):
        dx = (Xp[i] - x_mean)[:, None]
        cov += weights[i] * (Pp[i] + dx @ dx.T)
    
    return x_mean, cov

# ============== Processing Functions ==============

def process_window(window_idx, t_idx, W, T, observed_y, observed_yp, 
                  y0_e, y0_t, params):
    """
    Process a single analysis window with proper evidence calculation.


    window_idx : int, Current window index.
    t_idx : int, Starting time index for the window.
    W : int,  Window size (number of time points).
    T : array_like, Full time array.
    observed_y : array_like, Observed positions for full time series.
    observed_yp : array_like, Observed velocities for full time series.
    y0_e : array_like, shape (2,), Initial state for Euler method.
    y0_t : array_like, shape (2,), Initial state for Trapezoidal method.
    params : dict, Parameter dictionary.

    Returns

    """
    N = len(T)
    idx0 = t_idx
    idx1 = min(t_idx + W, N)
    
    t_slice = T[idx0:idx1]
    y_slice = observed_y[idx0:idx1]
    yp_slice = observed_yp[idx0:idx1]
    
    if len(t_slice) < 5:
        return None
    
    sigma_sl = np.full(len(t_slice), params['noise_level_s'])
    
    if window_idx == 0:
        print(f"\nProcessing window {window_idx+1}: indices=[{idx0}:{idx1}], size={len(t_slice)}")
    elif window_idx % 50 == 0:
        print(f"\nWindow {window_idx+1}, Progress: {t_idx/N*100:.1f}%")
    
    y0_window = np.array([y_slice[0], yp_slice[0]])
    
    # Euler method
    if window_idx == 0:
        print("Running ABC-SMC for Euler method...")
    thetaE, wE, _ = abc_smc_improved(
        euler_series, params['theta_prior'], y_slice, yp_slice,
        sigma_sl, t_slice, y0_window,
        params['num_particles'], params['num_generations'], params['k'],
        params['theta_min'], params['theta_max'], window_idx
    )
    
    x_e, P_e = ekf_window(
        thetaE[-1], wE[-1], y_slice, yp_slice, t_slice, params['k'],
        params['Q'], params['R'], y0_window, params['P0']
    )
    
    # Trapezoidal method
    if window_idx == 0:
        print("Running ABC-SMC for Trapezoidal method...")
    thetaT, wT, _ = abc_smc_improved(
        trapezoidal_series, params['theta_prior'], y_slice, yp_slice,
        sigma_sl, t_slice, y0_window,
        params['num_particles'], params['num_generations'], params['k'],
        params['theta_min'], params['theta_max'], window_idx
    )
    
    x_t, P_t = ekf_window(
        thetaT[-1], wT[-1], y_slice, yp_slice, t_slice, params['k'],
        params['Q'], params['R'], y0_window, params['P0']
    )
    
    # Get best parameter estimates
    theta_e_best = np.average(thetaE[-1], weights=wE[-1])
    theta_t_best = np.average(thetaT[-1], weights=wT[-1])
    
    # Compute evidence using importance sampling
    log_likes_e = []
    for i, theta in enumerate(thetaE[-1]):
        sim = euler_series(theta, params['k'], y0_window, t_slice)
        ll = log_likelihood_trajectory(sim, y_slice, yp_slice, sigma_sl)
        log_likes_e.append(ll)
    
    log_likes_t = []
    for i, theta in enumerate(thetaT[-1]):
        sim = trapezoidal_series(theta, params['k'], y0_window, t_slice)
        ll = log_likelihood_trajectory(sim, y_slice, yp_slice, sigma_sl)
        log_likes_t.append(ll)
    
    # Convert to numpy arrays
    log_likes_e = np.array(log_likes_e)
    log_likes_t = np.array(log_likes_t)
    
    # Use importance sampling to estimate log evidence
    log_weights_e = np.log(wE[-1] + 1e-300)
    log_weights_t = np.log(wT[-1] + 1e-300)
    
    # Check for numerical issues
    if np.all(np.isinf(log_likes_e)) or np.all(np.isinf(log_likes_t)):
        # Use alternative evidence estimate based on RMS errors
        sim_e = euler_series(theta_e_best, params['k'], y0_window, t_slice)
        sim_t = trapezoidal_series(theta_t_best, params['k'], y0_window, t_slice)
        
        if not (np.any(np.isnan(sim_e)) or np.any(np.isnan(sim_t))):
            rms_e = np.sqrt(np.mean((sim_e[0] - y_slice)**2 + (sim_e[1] - yp_slice)**2))
            rms_t = np.sqrt(np.mean((sim_t[0] - y_slice)**2 + (sim_t[1] - yp_slice)**2))
            
            # Convert RMS to evidence-like values
            Z_e = np.exp(-rms_e**2 / (2 * params['noise_level_s']**2))
            Z_t = np.exp(-rms_t**2 / (2 * params['noise_level_s']**2))
        else:
            Z_e = 1.0
            Z_t = 1.0
        
        if window_idx == 0:
            print(f"  Note: Using RMS-based evidence due to numerical limits")
    else:
        # Estimate log evidence
        log_Z_e = logsumexp(log_weights_e + log_likes_e)
        log_Z_t = logsumexp(log_weights_t + log_likes_t)
        
        # Normalize to prevent overflow
        max_log_Z = max(log_Z_e, log_Z_t)
        Z_e = np.exp(log_Z_e - max_log_Z)
        Z_t = np.exp(log_Z_t - max_log_Z)
    
    # Bayes factor
    BF_display = Z_t / (Z_e + 1e-10)
    BF_display = min(BF_display, 1000.0)
    BF_display = max(BF_display, 0.001)
    
    # Debug info for first window
    if window_idx == 0:
        print(f"  Parameters: γ_E={theta_e_best:.3f}, γ_T={theta_t_best:.3f} (true={0.3:.3f})")
        print(f"  Evidence: Z_E={Z_e:.3f}, Z_T={Z_t:.3f}, BF={BF_display:.1f}")
    
    error = np.linalg.norm(np.array([y_slice[-1], yp_slice[-1]]) - x_e)
    
    return {
        'window_center': T[(idx0 + idx1) // 2],
        'window_range': (idx0, idx1-1),
        'theta_e_mean': theta_e_best,
        'theta_t_mean': theta_t_best,
        'Z_e': Z_e,
        'Z_t': Z_t,
        'BF': BF_display,
        'x_e': x_e,
        'x_t': x_t,
        'error': error
    }

def adapt_window_size(W, errors, W_min, W_max, deltas):
    """
    Adapt window size based on error trend.

    W : int, Current window size.
    errors : list, History of errors.
    W_min : int
        Minimum window size.
    W_max : int, Maximum window size.
    deltas : int, Step size for adaptation.

    Returns

    W: int, New window size.
    """
    if len(errors) > 1:
        if errors[-1] > errors[-2] * 1.1:
            return max(W - deltas, W_min)
        else:
            return min(W + deltas, W_max)
    return W

def update_posteriors(Pe, Pt, Z_e, Z_t):
    """
    Update model posteriors using Bayes rule.

    Pe : float, Current posterior probability for Euler.
    Pt : float, Current posterior probability for Trapezoidal.
    Z_e : float,  Evidence for Euler model.
    Z_t : float,  Evidence for Trapezoidal model.
    
    Returns:
    (Pe_next, Pt_next): tuple, updated posteriors.
    """
    log_Pe = np.log(Pe + 1e-300)
    log_Pt = np.log(Pt + 1e-300)
    log_Z_e = np.log(Z_e + 1e-300)
    log_Z_t = np.log(Z_t + 1e-300)
    
    log_denom = logsumexp([log_Pe + log_Z_e, log_Pt + log_Z_t])
    
    Pe_next = np.exp(log_Pe + log_Z_e - log_denom)
    Pt_next = np.exp(log_Pt + log_Z_t - log_denom)
    
    if not np.isfinite(Pe_next) or not np.isfinite(Pt_next):
        return Pe, Pt
    
    return Pe_next, Pt_next

# ============== Plotting Functions ==============

def plot_observed_data(T, T_fine, sol_fine, observed_y, observed_yp, N):
    """Plot the observed data and true solution."""
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(T_fine, sol_fine.y[0], 'b-', label='True Position', alpha=0.7)
    obs_stride = max(1, N // 100)
    plt.scatter(T[::obs_stride], observed_y[::obs_stride], c='red', s=5, 
                label=f'Observations (every {obs_stride})', alpha=0.5, edgecolor='none')
    plt.xlabel('Time')
    plt.ylabel('Position')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.title('Damped Oscillator Position (5000 measurements)')
    
    plt.subplot(1, 2, 2)
    plt.plot(T_fine, sol_fine.y[1], 'b-', label='True Velocity', alpha=0.7)
    plt.scatter(T[::obs_stride], observed_yp[::obs_stride], c='red', s=5,
                label=f'Observations (every {obs_stride})', alpha=0.5, edgecolor='none')
    plt.xlabel('Time')
    plt.ylabel('Velocity')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.title('Damped Oscillator Velocity')
    
    plt.tight_layout()
    plt.show()

def plot_model_posteriors(P_e_hist, P_t_hist, window_centers):
    """Plot evolution of model posteriors."""
    if len(P_e_hist) > 1:
        plt.figure(figsize=(10, 5))
        posterior_times = [0] + window_centers
        plt.plot(posterior_times[:len(P_e_hist)], P_e_hist, 'o-', label='P(Euler|Data)', 
                linewidth=1, markersize=3, color='red', alpha=0.7)
        plt.plot(posterior_times[:len(P_t_hist)], P_t_hist, 's-', label='P(Trapezoidal|Data)', 
                linewidth=1, markersize=3, color='blue', alpha=0.7)
        plt.xlabel('Measurement Time')
        plt.ylabel('Model Posterior Probability')
        plt.title('Evolution of Model Posteriors (updated after each window)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(-0.05, 1.05)
        plt.tight_layout()
        plt.show()

def plot_evidence_values(Z_e_hist, Z_t_hist, window_centers):
    """Plot model evidence evolution."""
    if len(Z_e_hist) > 0:
        plt.figure(figsize=(10, 5))
        
        plt.plot(window_centers, Z_e_hist, 'o-', label='Forward Euler', 
                linewidth=1, markersize=3, color='blue', alpha=0.7)
        plt.plot(window_centers, Z_t_hist, 's-', label='Trapezoidal rule', 
                linewidth=1, markersize=3, color='orange', alpha=0.7)
        plt.ylabel('Model evidence (normalized)')
        plt.xlabel('Measurement Time')
        plt.title('Model Evidence Evolution (computed over each window)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.yscale('log')  # Use log scale to see variations better
        plt.tight_layout()
        plt.show()

def plot_bayes_factors(BF_list, window_centers):
    """Plot Bayes factor evolution."""
    if len(BF_list) > 0:
        clean_BF = [bf if (np.isfinite(bf) and bf > 0) else 1.0 for bf in BF_list]
        
        plt.figure(figsize=(10, 5))
        plt.plot(window_centers, clean_BF, 'o-', label='Bayes Factor (Z_t/Z_e)', 
                linewidth=1, markersize=3, color='green', alpha=0.7)
        plt.axhline(y=1, color='k', linestyle='--', alpha=0.5, label='No preference')
        plt.axhline(y=3, color='r', linestyle=':', alpha=0.5, label='Strong evidence')
        plt.axhline(y=10, color='r', linestyle=':', alpha=0.3, label='Very strong evidence')
        plt.xlabel('Measurement Time')
        plt.ylabel('Bayes Factor (log scale)')
        plt.title('Bayes Factor Evolution (Trapezoidal/Euler)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        plt.ylim(0.1, 1000)
        plt.tight_layout()
        plt.show()

def plot_parameter_estimates(theta_e_means, theta_t_means, window_centers, gamma_true):
    """Plot parameter estimation evolution."""
    if len(theta_e_means) > 0:
        plt.figure(figsize=(10, 5))
        plt.plot(window_centers, theta_e_means, 'o-', label='Euler estimates', 
                linewidth=1, markersize=3, color='red', alpha=0.7)
        plt.plot(window_centers, theta_t_means, 's-', label='Trapezoidal estimates', 
                linewidth=1, markersize=3, color='blue', alpha=0.7)
        plt.axhline(y=gamma_true, color='k', linestyle='--', alpha=0.5, 
                   label=f'True value ({gamma_true})')
        plt.xlabel('Measurement Time')
        plt.ylabel('Damping Parameter (γ)')
        plt.title('Parameter Estimation: Euler vs Trapezoidal (weighted mean of particles)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        all_estimates = theta_e_means + theta_t_means
        estimate_range = max(all_estimates) - min(all_estimates)
        if estimate_range > 1e-6:
            y_margin = max(0.05, 0.1 * estimate_range)
            plt.ylim(min(all_estimates) - y_margin, max(all_estimates) + y_margin)
        else:
            plt.ylim(gamma_true - 0.05, gamma_true + 0.05)
        plt.tight_layout()
        plt.show()

def plot_window_coverage(T, window_ranges, N):
    """Plot measurement times and window coverage."""
    plt.figure(figsize=(14, 6))
    
    for i, (start_idx, end_idx) in enumerate(window_ranges):
        color = plt.cm.viridis(i / len(window_ranges))
        plt.fill_between(T[start_idx:end_idx+1], 0, 1, alpha=0.3, color=color)
        plt.axvline(T[start_idx], color=color, alpha=0.5, linewidth=0.5)
        if i == len(window_ranges) - 1:
            plt.axvline(T[end_idx], color=color, alpha=0.5, linewidth=0.5)
    
    boundary_indices = []
    for start_idx, end_idx in window_ranges:
        if start_idx not in boundary_indices:
            boundary_indices.append(start_idx)
        if end_idx not in boundary_indices:
            boundary_indices.append(end_idx)
    
    plt.scatter(T[boundary_indices], np.ones_like(boundary_indices) * 0.5, 
                c='black', s=20, marker='|', label='Window boundaries')
    
    plt.ylim(0, 1.1)
    plt.xlabel('Time (seconds)')
    plt.ylabel('Window Coverage')
    plt.title(f'{N} Measurements Analyzed in {len(window_ranges)} Connected Windows')
    plt.legend()
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.show()

def print_summary(N, window_idx, window_sizes, P_e_hist, P_t_hist, BF_list, 
                  theta_e_means, theta_t_means, gamma_true):
    """Print summary statistics."""
    print("\n=== Summary ===")
    print(f"Total measurement points: {N}")
    print(f"Total windows processed: {window_idx}")
    
    if len(window_sizes) > 0:
        coverage = sum(window_sizes) / N * 100
        print(f"Coverage: {coverage:.1f}% of time series")
        print(f"Window sizes: min={min(window_sizes)}, max={max(window_sizes)}, "
              f"mean={np.mean(window_sizes):.1f}")
    
    print(f"Final model posteriors: P(Euler)={P_e_hist[-1]:.3f}, "
          f"P(Trapezoidal)={P_t_hist[-1]:.3f}")
    
    if len(theta_e_means) > 0 and len(theta_t_means) > 0:
        print(f"\nParameter estimates (true value = {gamma_true}):")
        print(f"  Euler: mean={np.mean(theta_e_means):.4f}, std={np.std(theta_e_means):.4f}")
        print(f"  Trapezoidal: mean={np.mean(theta_t_means):.4f}, std={np.std(theta_t_means):.4f}")
    
    if len(BF_list) > 0:
        clean_BF = [bf for bf in BF_list if (np.isfinite(bf) and bf > 0)]
        if len(clean_BF) > 0:
            print(f"\nBayes factors (Trapezoidal/Euler):")
            print(f"  Geometric mean: {np.exp(np.mean(np.log(clean_BF))):.3f}")
            print(f"  Median: {np.median(clean_BF):.3f}")
            print(f"  Min/Max: {min(clean_BF):.3f} / {max(clean_BF):.3f}")
            print(f"  Percentage > 3 (strong evidence): {sum(bf > 3 for bf in clean_BF)/len(clean_BF)*100:.1f}%")
            print(f"  Percentage > 10 (very strong evidence): {sum(bf > 10 for bf in clean_BF)/len(clean_BF)*100:.1f}%")
    
    print("\nAnalysis complete!")

# ============== Main Program ==============

if __name__ == '__main__':
    # Set random seed
    seed = 42
    np.random.seed(seed)
    
    # System parameters
    params = {
        'm': 1,
        'k': 5,
        'gamma_true': 0.3,
        'initial_conditions': [1, 0],
        'noise_level_s': 0.05,
        'num_particles': 100,
        'num_generations': 8,
        'theta_min': 0.1,
        'theta_max': 0.5,
        'W_boot': 30,
        'W_min': 10,
        'W_max': 100,
        'deltas': 5,
        'max_windows': 200
    }
    
    # Create prior
    params['theta_prior'] = np.linspace(params['theta_min'], params['theta_max'], 100)
    
    # EKF parameters
    params['P0'] = np.eye(2) * 0.1
    params['Q'] = np.eye(2) * 1e-5
    params['R'] = np.eye(2) * params['noise_level_s']**2
    
    # Generate true solution
    T_fine = np.linspace(0, 10, 1000)
    sol_fine = scipy.integrate.solve_ivp(
        damped_oscillator, [0, 10], params['initial_conditions'], 
        args=(params['gamma_true'], params['k']), t_eval=T_fine
    )
    
    # Generate measurements
    T = np.linspace(0, 10, 5000)
    N = len(T)
    dt = T[1] - T[0]
    
    # Interpolate and add noise
    true_y = np.interp(T, T_fine, sol_fine.y[0])
    true_yp = np.interp(T, T_fine, sol_fine.y[1])
    observed_y = true_y + np.random.normal(0, params['noise_level_s'], N)
    observed_yp = true_yp + np.random.normal(0, params['noise_level_s'], N)
    
    # Print setup
    print("=== Bayesian Model Selection: Euler vs Trapezoidal Method ===")
    print(f"System: Damped oscillator with k={params['k']}, m={params['m']}")
    print(f"Number of measurement points: {N}")
    print(f"Time span: {T[0]:.1f} to {T[-1]:.1f} seconds")
    print(f"Measurement interval: {dt:.4f} seconds")
    print(f"Noise level: {params['noise_level_s']:.3f}")
    print(f"True damping parameter: {params['gamma_true']:.3f}")
    print(f"Prior range: [{params['theta_min']:.2f}, {params['theta_max']:.2f}]")
    if params['theta_max'] - params['theta_min'] > 0.5:
        print("  Note: Wide prior range may lead to convergence challenges")
    print(f"ABC-SMC: {params['num_particles']} particles, {params['num_generations']} generations")
    print(f"Window configuration: CONNECTED (non-overlapping)")
    print("Analysis approach: Each window is analyzed independently")
    print("Plotting: Results shown at window centers represent full window analysis")
    print()
    
    # Initialize histories
    P_e_hist, P_t_hist = [0.5], [0.5]
    Z_e_hist, Z_t_hist = [], []
    X_e_hist, X_t_hist = [], []
    err_hist = []
    window_sizes = []
    window_centers = []
    window_ranges = []
    BF_list = []
    theta_e_means = []
    theta_t_means = []
    
    # Initial states
    y0_e = np.array(params['initial_conditions'], dtype=float)
    y0_t = np.array(params['initial_conditions'], dtype=float)
    
    # Process windows
    W = params['W_boot']
    window_idx = 0
    t_idx = 0
    
    print("Starting window-based analysis with CONNECTED windows...")
    print("Each point in the plots represents the Bayesian analysis over an entire window")
    print()
    
    while t_idx < N and window_idx < params['max_windows']:
        # Process window
        result = process_window(
            window_idx, t_idx, W, T, observed_y, observed_yp,
            y0_e, y0_t, params
        )
        
        if result is None:
            break
        
        # Store results
        window_sizes.append(W)
        window_centers.append(result['window_center'])
        window_ranges.append(result['window_range'])
        theta_e_means.append(result['theta_e_mean'])
        theta_t_means.append(result['theta_t_mean'])
        Z_e_hist.append(result['Z_e'])
        Z_t_hist.append(result['Z_t'])
        BF_list.append(result['BF'])
        X_e_hist.append(result['x_e'])
        X_t_hist.append(result['x_t'])
        err_hist.append(result['error'])
        
        # Update posteriors
        Pe_next, Pt_next = update_posteriors(
            P_e_hist[-1], P_t_hist[-1], 
            result['Z_e'], result['Z_t']
        )
        P_e_hist.append(Pe_next)
        P_t_hist.append(Pt_next)
        
        # Update initial conditions for next window
        y0_e = result['x_e']
        y0_t = result['x_t']
        
        # Adapt window size
        W = adapt_window_size(W, err_hist, params['W_min'], params['W_max'], params['deltas'])
        
        # Move to next window (CONNECTED, not overlapping)
        window_idx += 1
        t_idx += W
    
    if len(window_sizes) > 0:
        coverage = window_idx * np.mean(window_sizes) / N * 100
        print(f"\n✓ Completed {window_idx} windows ({coverage:.1f}% coverage)")
    else:
        print(f"\n✓ Completed {window_idx} windows")
    
    # Generate all plots
    plot_observed_data(T, T_fine, sol_fine, observed_y, observed_yp, N)
    plot_model_posteriors(P_e_hist, P_t_hist, window_centers)
    plot_evidence_values(Z_e_hist, Z_t_hist, window_centers)
    plot_bayes_factors(BF_list, window_centers)
    plot_parameter_estimates(theta_e_means, theta_t_means, window_centers, params['gamma_true'])
    plot_window_coverage(T, window_ranges, N)
    
    # Print summary
    print_summary(N, window_idx, window_sizes, P_e_hist, P_t_hist, BF_list,
                  theta_e_means, theta_t_means, params['gamma_true'])
