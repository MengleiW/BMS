# -*- coding: utf-8 -*-
"""
Description: Determine the model evidence for two classes of atomic models,
    using the MCMC samples of the estimated model parameters (tht) and
    associated samples from the model outputs y = f(tht,M).
   
    Each model output is a 9-dimensional vector representing the difference
    between the computed and the reference NIST energies.
   
    The parameters are 2-dimensional vectors representing a subset of orbital
    scaling parameters with an assumed uniform prior on [0.7,1.3]^2.


    We compute the model evidence using
   
   
    1. Importance sampling
    
    2. Harmonic Mean
    
    3. Reverse important sampling with t-distributions and multivariate normal distribution as auxiliary normalized function.

    4. Two-stage umbrella
   
Created on Mon Oct  2 9:34:22 2023

@author: whisk
"""

import numpy as np
import matplotlib.pyplot as plt
import math
import pickle
import scipy.special

import random

def uniform_prior(theta, theta_bounds):
    """
    Description: To find the uniform prior.

    Inputs:
       
        theta: theta_i stands for the variable of interest
       
        theta_bounds: (d,2) array whose ith row gives the minimum and maximum
            value of theta_i
       
    Outputs:
   
         prior: (1,1) approximation of normalized prior

       
       
     Modified:
   
         10/07/2023 (Menglei Wang)
           
    """
   
    # Dimensions of the sampled parameter
    N, d = theta.shape
   
    # Area of the region
    A = np.prod(theta_bounds[:,1]-theta_bounds[:,0])
   
    # Determine what samples lie within region
    in_region = np.ones(N)
    for di in range(d):
        in_region *= (theta_bounds[di,0]<=theta[:,0])\
                    *(theta[:,1]<=theta_bounds[di,1])
                   
    # Multiply by Area
    prior = in_region/A
   
    return prior



def gauss_log_likelihood(y, sigma):
    """
    Description: Evaluate the multivariate Gaussian log likelihood function
        associated with model outputs

    Inputs:
       
        y : double, (N,m) array of sample model outputs.
       
        sigma: double >0, the standard diviation of Multivariate normal
            distribution representing the experimental error.
       
       
    Outputs:
   
        l: double, (N,) vector of log-likelihood values
           
           
    Modified:
       
        10/08/2023 (Menglei Wang)
           
    """
    # Data size
    N, m = y.shape
   
    # Gaussian log-likelihood
    ll = -0.5*m*np.log(2*np.pi) - 0.5*m*np.log(sigma**2) \
        - 0.5/(sigma**2)*np.sum(y**2,axis=1)
    
    return ll
   
def HM_FindZ(log_likelihood,y):
    """
    Description: To find the Z = nomalizing constant using Importat sampling method given liklihood and prior

    Inputs:
       
       
        log_Likelihood: Single, (1,1) , l(y|theta_i, M) where theta_i stands for variable of interest , y stands for the data and
        M is our current model.
       
       
        Outputs:
       
            Zhat: (1,1) approximatioin of nomalized posterior.

           
           
        Modified:
       
            10/05/2023 (Menglei Wang)
           
        """
    # Data size
    N, m = y.shape   
    
    #unlog the log_likehood
    likelihood = np.exp(log_likelihood)
    
    #calculate Zhat
    Zhat = 1/((np.sum(1/likelihood))/N)
    
    return Zhat
   
def Is_FindZ(log_likelihood, prior):
    """
    Description: To find the Z = nomalizing constant using Importat sampling method given liklihood and prior

    Inputs:
       
       
        log_Likelihood: Single, (1,1) , l(y|theta_i, M) where theta_i stands for variable of interest , y stands for the data and
        M is our current model.
       
        prior : Single, (1,N) , g(theta_i) where theta_i stands for variable of interest.
       
       
        Outputs:
       
            Zhat: (1,1) approximatioin of nomalized posterior.

           
           
        Modified:
       
            10/05/2023 (Menglei Wang)
           
        """

    # Data size
    N, m = y.shape

    #unlog the log_likehood
    likelihood = np.exp(log_likelihood)
    
    # Summing the product of prior and likelihoo
    phon = np.sum(prior * likelihood) 
    print('phon =', phon)
    # Calculating the normalized posterior
    Zhat = np.sum((prior * likelihood)/phon)
    print('Zhat =', Zhat)
    
    return Zhat
 
def RIS_FindZ(log_likelihood, prior,y):
    """
    Description: To find the Z = nomalizing constant using Importat sampling method given liklihood and prior

    Inputs:
       
       
        log_Likelihood: Single, (1,1) , l(y|theta_i, M) where theta_i stands for variable of interest , y stands for the data and
        M is our current model.
       
        prior : Single, (1,N) , g(theta_i) where theta_i stands for variable of interest.
       
       
        Outputs:
       
            Zhat: (1,1) approximatioin of nomalized posterior.

           
           
        Modified:
       
            10/15/2023 (Menglei Wang)
           
        """

    # Data size
    N, m = y.shape

    #unlog the log_likehood
    likelihood = np.exp(log_likelihood)
    
    # Using multivariate gaussian as my auxiliry function
    #f = -0.5*m*np.log(2*np.pi) - 0.5*m*np.log(sigma**2) \
        #- 0.5/(sigma**2)*np.sum(y**2,axis=1)
        
    #Using t-distributions as auxiliary normalized function
    t = 100
    f = np.log(scipy.special.gammaln((t+1)/2))+(-(t+1)/2)*np.log(1+(m**2)/t)-np.log(((t*math.pi)**0.5)*scipy.special.gammaln(t/2))
    print('f =', f)
    if f < 0:
        f =-f 
    # Calculating the normalized posterior
    Zhat = 1/(np.sum(f/(prior * likelihood))/N)
    print('Zhat =', Zhat)
    
    return Zhat
 
def HMModel_compaire (data,sigma):
    """
    Description: To Compaire models using normalizing constant approximated by Hermonic Mean smapling

    Inputs:
        data: (2,2) The data generated by a MCMC = {'model01': {'tht': tht_for_model01, 'y': y_vals_for_model01 }, 
              'model02': {'tht': tht_for_model02, 'y': y_vals_for_model02 }} 
       
        sigma: double >0, the standard diviation of Multivariate normal
               distribution representing the experimental error.

       
       
        Outputs:
       
            C: (1,1) Ratios between the normalizing constants of two models.

            Graph: A pie graph of the ratio C.
           
        Modified:
       
            10/12/2023 (Menglei Wang)
           
        """
    #extrac data
    tht1 = data['model01']['tht']
    tht2 = data['model02']['tht']
    y1 = data['model01']['y']
    y2 = data['model02']['y']
    theta_bounds = np.array([[0.7,1.3],[0.7,1.3]])
   
    # Data size
    N, m = y1.shape
    
    #Finding Z1
    likelihood1 = gauss_log_likelihood(y1,sigma)
    Z1 = HM_FindZ(likelihood1,y1)
    
   #Finding Z2
    likelihood2 = gauss_log_likelihood(y2,sigma)
    Z2 = HM_FindZ(likelihood2,y2)
    
    #Comparie the nomalizing constant
    C = Z2/Z1
    print('c = ' , C)
    
    #graphing
    plt.pie([Z2,Z1], labels= [Z2,Z1], colors = ["red","blue"])
    plt.annotate('Red is Z2', xy=(-1.1,0.8))
    plt.annotate('Blue is Z1', xy=(-1.1, 0.9)) 
    plt.title('Z1,Z2')
    plt.tight_layout()
    plt.show()
   

def ISModel_compaire (data,sigma):
    """
    Description: To Compaire models using normalizing constant approximated by Reverse important smapling

    Inputs:
        data: (2,2) The data generated by a MCMC = {'model01': {'tht': tht_for_model01, 'y': y_vals_for_model01 }, 
              'model02': {'tht': tht_for_model02, 'y': y_vals_for_model02 }} 
       
        sigma: double >0, the standard diviation of Multivariate normal
               distribution representing the experimental error.

       
       
        Outputs:
       
            C: (1,1) Ratios between the normalizing constants of two models.

           
           
        Modified:
       
            10/12/2023 (Menglei Wang)
           
        """
    
  
    
    #extrac data
    tht1 = data['model01']['tht']
    tht2 = data['model02']['tht']
    y1 = data['model01']['y']
    y2 = data['model02']['y']
    
    # Data size
    N, m = y1.shape
    
    #Finding Z1
    prior1 = uniform_prior(tht1)
    likelihood1 = gauss_log_likelihood(y1,sigma,N)
    Z1 = Is_FindZ(N,likelihood1, prior1)
    
   #Finding Z2
    prior2 = uniform_prior(tht2)
    likelihood2 = gauss_log_likelihood(y2,sigma,N)
    Z2 = Is_FindZ(N,likelihood2, prior2)
   
    C = Z2/Z1
    print('c = ' , C)
    
def RisModel_compaire (data,sigma):
    """
    Description: To Compaire models using normalizing constant approximated by Hermonic Mean smapling

    Inputs:
        data: (2,2) The data generated by a MCMC = {'model01': {'tht': tht_for_model01, 'y': y_vals_for_model01 }, 
              'model02': {'tht': tht_for_model02, 'y': y_vals_for_model02 }} 
       
        sigma: double >0, the standard diviation of Multivariate normal
               distribution representing the experimental error.

       
       
        Outputs:
       
            C: (1,1) Ratios between the normalizing constants of two models.

            Graph: A pie graph of the ratio C.
           
        Modified:
       
            10/12/2023 (Menglei Wang)
           
        """
    #extrac data
    tht1 = data['model01']['tht']
    tht2 = data['model02']['tht']
    y1 = data['model01']['y']
    y2 = data['model02']['y']
    theta_bounds = np.array([[0.7,1.3],[0.7,1.3]])
   
    # Data size
    N, m = y1.shape
    
    
    #Finding Z1
    log_likelihood1 = gauss_log_likelihood(y1,sigma)
    prior1 = uniform_prior(tht1, theta_bounds)
    Z1 = RIS_FindZ(log_likelihood1,prior1,y1)
    
   #Finding Z2
    log_likelihood2 = gauss_log_likelihood(y2,sigma)
    prior2 = uniform_prior(tht2, theta_bounds)
    Z2 = RIS_FindZ(log_likelihood2,prior2,y2)
    
    #Comparie the nomalizing constant
    C = Z2/Z1
    print('c = ' , C)
    
    #graphing
    plt.pie([Z2,Z1], labels= [Z2,Z1],colors = ["red","blue"])
    plt.annotate('Red is Z2', xy=(-1.1,0.8))
    plt.annotate('Blue is Z1', xy=(-1.1, 0.9)) 
    plt.title('Z1,Z2')
    plt.tight_layout()
    plt.show()

def Metropolis_hasting(M,m,sigma,y,q3):
    """
    Description: Use the Metropolis-Hastings sampler to generate a sample froma Rayleigh distribution.

    Inputs:
       
        sigma: double >0, the standard diviation of Multivariate normal
               distribution representing the experimental error.
       
        m: dimension of the variables.
        
        M: number of samples 
        
        y: data from either first model or second moddel (y1,y2)
        
        q3: target distrabution
       
    Outputs:
   
         samples: Theta_i sample gather from the target distribution.

       
       
     Modified:
   
         10/22/2023 (Menglei Wang)
              
    """        

    #Set empty parameters
    samples = []
    X_t = np.ones(m)
    
    for i in range(5):
        # Propose a new state from multivariate distribution 
        Y = np.random.multivariate_normal(X_t, sigma**2 * np.eye(m))
        print('Y=',Y)
        print('q3=',q3(Y, sigma,sigma**2 * np.eye(9)))
        #calculate acceptance rate alpha ratio, reduction due to symmetric proposal distributions.
        r = q3(Y,sigma,sigma**2 * np.eye(m))/q3(X_t,np.mean(y),sigma**2 * np.eye(m))
        print('r=',r)
        
        alpha = np.minimum(1, r)
        print('alpha=',alpha)
        if np.random.random() < alpha:
            X_t = Y
            #print(X_t)
        samples.append(X_t)
    
    
    
    
def Umbrella_sampling (NS,data, sigma ,q3 , theta_bounds):   
    """
    Description: To Compaire models using normalizing constant ratio approximated by Multi-stage(M stage) umbrella smapling. 
    Drawing sample using MCMC Metropolis-Hastings sampler

    Inputs:
        
        Ns:(1,1) number of stages
        
        data: (2,2) The data generated by a MCMC = {'model01': {'tht': tht_for_model01, 'y': y_vals_for_model01 }, 
              'model02': {'tht': tht_for_model02, 'y': y_vals_for_model02 }} 
       
        sigma: double >0, the standard diviation of Multivariate normal
               distribution representing the experimental error.

       
       
        Outputs:
       
            C: (1,1) Ratios between the normalizing constants of two models.

           
        Modified:
       
            10/20/2023 (Menglei Wang)
           
        """
        
    # Data size
    N, m = y.shape  
    #extrac data
    
    y1 = data['model01']['y']
    
    #print(y1)
    y2 = data['model02']['y']
    
    # Data size
    N, m = y1.shape
    #cov = sigma**2 * np.eye(m)
    #print('cov=', cov)
    
    #Sampling using mertropolis hasting MCMC method.
    tht1 = Metropolis_hasting(N,m,sigma,y1,q3)
    print('tht1=',tht1)
    tht2 = Metropolis_hasting(N,m,sigma,y2,q3)
    
    
    #Finding Z1
    prior1 = uniform_prior(tht1, theta_bounds)
    likelihood1 = gauss_log_likelihood(y1,sigma,N)
    q11 = prior1*likelihood1
    
    #Finding Z2
    prior2 = uniform_prior(tht2, theta_bounds)
    likelihood2 = gauss_log_likelihood(y2,sigma,N)
    q22 = prior2*likelihood2
    
    #Let q12 and q21 = q3, there for the sum of each denominator become N and cancles.
    rh1 = np.sum(q11/q22)
        
    #Update q3
    q3 =  q11 - rh1 * q22
        
    return q3 

if __name__ == '__main__':
    atomic_data_pickle_path = 'C:\\Users\\whisk\\atomic_data.pickle'
    #atomic_data_path =  '../data/atomic_data.pickle'
    with open('atomic_data.pickle', 'rb')  as f:
       data = pickle.load(f)
    NS = 2   
    sigma = 0.1
    #HMModel_compaire (data,sigma)
    #RisModel_compaire (data,sigma)
   
    
    y = data['model01']['y']
   
    
    #data = {
        #'model01': {'y': np.random.rand(1, 9)},
        #'model02': {'y': np.random.rand(1, 9)}
         #}
    #q3 = lambda x, mean, covariance: scipy.stats.multivariate_normal.pdf(x, mean=mean, cov=np.atleast_2d(covariance))
    q3 = lambda m, sigma, y: -0.5 * np.sum(np.log(2 * np.pi * sigma**2) + (y ** 2) / (sigma**2))
    


    theta_bounds = np.array([[0.7,1.3],[0.7,1.3]])
    #print(y.shape)
    US = Umbrella_sampling (NS,data, sigma, q3 ,theta_bounds )
    
    #ll = gauss_log_likelihood(y, sigma)
    #theta = data['model02']['tht']
    
    
    #pp = uniform_prior(theta,theta_bounds)
    #Is_FindZ(ll, pp)
    #print('pp=',data.shape)
   
   
    #ISModel_compaire(data,0.1,9)
