import numpy as np
import matplotlib.pyplot as plt
import math
import pickle
import scipy.special

import random

def uniform_prior(theta, theta_bounds):

   
    # Dimensions of the sampled parameter
    N, d = theta.shape
   
    # Area of the region
    A = np.prod(theta_bounds[:,1]-theta_bounds[:,0])
   
    # Determine what samples lie within region
    in_region = np.ones(N)
    for di in range(d):
        in_region *= (theta_bounds[di,0]<=theta[:,0])\
                    *(theta[:,1]<=theta_bounds[di,1])
                   
    # Multiply by Area
    prior = in_region/A
   
    return prior



def gauss_log_likelihood(y, sigma):

    # Data size
    N, m = y.shape
   
    # Gaussian log-likelihood
    ll = -0.5*m*np.log(2*np.pi) - 0.5*m*np.log(sigma**2) \
        - 0.5/(sigma**2)*np.sum(y**2,axis=1)
    
    return ll
def Metropolis_hasting(weights,M,m,target_function,proposal_function ):#= 'Gaussian'):


    #Set empty parameters
    theta1 = []
    theta2 = []
    X_t = np.ones(2*m)
    
    #if proposal_function == 'Gaussian':
        #proposal_function = x: np.random.multivariate_normal(x, cov=np.eye(len(x)) * 0.1)

    for i in range(M):
        # Propose a new state from multivariate distribution 
        Y = proposal_function(X_t)
        Y1, Y2 = Y[:m], Y[m:]
        #calculate acceptance rate alpha ratio, reduction due to symmetric proposal distributions.
        r = target_function(Y)/target_function(X_t) * weights[i % len(weights)]
       # print('r=',r)
        
        alpha = np.minimum(1, r)
        #print('alpha=',alpha)
        
        if np.random.random() < alpha:
            X_t = Y
            theta1.append(Y1)
            theta2.append(Y2)
        else:
            
            theta1.append(X_t[:m])
            theta2.append(X_t[m:])
    theta1 = np.array([arr.flatten() for arr in theta1])
    theta2 = np.array([arr.flatten() for arr in theta2])
    #print(theta2)
    return theta1, theta2
    
    
def Umbrella_sampling (NS,data, sigma ,target_function, proposal_function ):   


    
    #extrac data
    y1 = data['model01']['y']
    y2 = data['model02']['y']
    tht2 = data['model02']['tht']

    # Data size
    N, m = tht2.shape
   
    weight = np.ones(N)
    #Sampling using mertropolis hasting MCMC method.
    tht1,tht2  = Metropolis_hasting(weight,N,m,target_function,proposal_function)

    #tht2 = Metropolis_hasting(N,m,target_function,proposal_function)
    theta_bounds = np.array([[0.7,1.3],[0.7,1.3]])
    
    
    
    #Finding Z1
    prior1 = uniform_prior(tht1, theta_bounds)
    likelihood1 = gauss_log_likelihood(y1,sigma)
    q1 = np.array(prior1*likelihood1)
    
    #Finding Z2
    prior2 = uniform_prior(tht2, theta_bounds)
    likelihood2 = gauss_log_likelihood(y2,sigma)
    q2 = np.array(prior2*likelihood2)
    
    
    
    
    #Let q12 and q21 = q3, there for the sum of each denominator become N and cancles.
    epsilon = 1e-10
    q1 =np.log(np.maximum(q1, epsilon))
    q2 = np.log(np.maximum(q2, epsilon))
    
    
    rh = np.exp(scipy.special.logsumexp(q1 - q2))
    
        
    #Update q3
    
    weight = np.abs(q1 - rh * q2)
    
    for i in range(NS):
        #Sampling using mertropolis hasting MCMC method.
        tht1,tht2  = Metropolis_hasting(weight,N,m,target_function,proposal_function)

        #tht2 = Metropolis_hasting(N,m,target_function,proposal_function)
        theta_bounds = np.array([[0.7,1.3],[0.7,1.3]])
        
        
        
        #Finding Z1
        prior1 = uniform_prior(tht1, theta_bounds)
        likelihood1 = gauss_log_likelihood(y1,sigma)
        q1 = np.array(prior1*likelihood1)
        
        #Finding Z2
        prior2 = uniform_prior(tht2, theta_bounds)
        likelihood2 = gauss_log_likelihood(y2,sigma)
        q2 = np.array(prior2*likelihood2)
        
        #Let q12 and q21 = q3, there for the sum of each denominator become N and cancles.
        epsilon = 1e-10
        q1 = np.maximum(q1, epsilon)
        q2 = np.maximum(q2, epsilon)
        
        q1 = np.log(q1)
        q2 = np.log(q2)
        rh = np.exp(scipy.special.logsumexp(q1 - q2))
            
        #Update weight
        
        weight = np.abs(q1 - rh * q2)
    
    
    #nomalize q1 and q2
    Z1 = np.sum(np.exp(q1))
    Z2 = np.sum(np.exp(q2))
    print('Z1=',Z1)
    #h= rh +1
    #Z2=rh/h
    #Z1 = 1/h
    
    
    #plt.plot([q2,q1])
    #plt.title('Z1,Z2')
    #plt.tight_layout()
    #plt.show()  
    #graphing
    plt.pie([Z2,Z1], labels= [Z2,Z1],colors = ["red","blue"])
    
    plt.annotate('Red is Z2', xy=(-1.1,0.8))
    plt.annotate('Blue is Z1', xy=(-1.1, 0.9)) 
    plt.title('Z1,Z2')
    plt.tight_layout()
    plt.show()    
    return 

if __name__ == '__main__':
    atomic_data_pickle_path = 'C:\\Users\\whisk\\atomic_data.pickle'
    #atomic_data_path =  '../data/atomic_data.pickle'
    with open('atomic_data.pickle', 'rb')  as f:
       data = pickle.load(f)
    NS = 10   
    sigma = 0.1
    #HMModel_compaire (data,sigma)
    #RisModel_compaire (data,sigma)
                                                            
    tht2 = data['model02']['tht']

    

    
    target_function = lambda x : scipy.stats.multivariate_normal.logpdf(x, cov=np.eye(len(x)) * 0.1)
    
    proposal_function  = lambda x: np.random.multivariate_normal(x, cov=np.eye(len(x)) * 0.1)
    
    #print(y.shape)
    US = Umbrella_sampling (NS,data, sigma ,target_function, proposal_function   )
